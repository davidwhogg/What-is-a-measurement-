% Copyright 2025 David W. Hogg. All rights reserved.

% to-do items
% -----------
% - explicitly address the difference between a measurement and an estimator, or the output of an estimator. What is an estimate?
% - make sure it is abundantly clear that the use of anyone's data involves adopting myriad assumptions. Should there be an assumptions and subjectivity section?
% - add in the "worked example" in "basic_example.ipynb" perhaps?
% - Make it clear who the audience is: People who know how to do data analysis, who know basic statistics and have used it. Then audit for that level.
% - Will Farr argument: Posterior samplings are such an elegant way of delivering results. I don't disagree with that point.
% - give to friendlies for feedback -- especially Rix, Margossian, Marshall, Chandra, Brewer, Jaffe, Modi, Heinrich, DFM, Peebles, Beane, Audenaert, Lindegren, Malz, APW, DNS, Trotta ?
% - make a plan for how we cite arXiv papers and make it consistent over the whole bibliography.
% - Fix the et al formatting in the bibliography. Heck. Fix all the formatting in the bibliography.
% - Check Fisher citation; also good for maximum-likelihood??
% - This paper is very relevant and ought to be cited: https://arxiv.org/pdf/2408.07700
% - Are we clear about what we are suggesting, positively, in each section?
% - Are we clear about what constitutes "data" and what we mean by "parameters"? Example of nucleosynthetic parameters vs element abundance parameters?
% - community norms loom large in the abstract. But are they important enough in the paper?
% - Example of LIGO populations analyses?! Should be in the Bayes part, maybe?
% - This paper opens with the issue of community norms / agreements, but then never does anything with that? Should it get specific?
% - Is it clear: This paper is for someone who wants to think very rigorously about the measurement they are making.

% style notes
% -----------
% - Audit for second-person language; maybe make it more third-person where possible?
% - Is first person "I" or "we"? I think "I" but use "we" when talking about shared goals with the reader.
% - Use \emph{} every time I introduce a new term of jargon that is important to know.
% - Be careful with "set" vs "list" vs "vector" vs "matrix".
% - Use LF in abstract, maybe, but not the main text, which should read clean.
% - Footnotes before or after commas and periods?
% - "data" is a mass noun like "grass" or "hair."
% - Should all pdfs be conditioned on I? I think so; fix that and audit for it.

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper]{geometry}
\usepackage{setspace}
\usepackage{amsmath, amsfonts, mathrsfs}

% typesetting issues
\setstretch{1.08}
\addtolength{\topmargin}{-0.30in}
\addtolength{\textheight}{1.60in}
\setlength{\textwidth}{5.0in}
\setlength{\oddsidemargin}{0.5\paperwidth}\addtolength{\oddsidemargin}{-1.0in}\addtolength{\oddsidemargin}{-0.5\textwidth}
\pagestyle{myheadings}
\markboth{foo}{\sffamily Hogg / What is a measurement?}
\renewcommand{\newblock}{} % this adjusts the bibliography style.
\frenchspacing\sloppy\sloppypar\raggedbottom

% text macros
\renewcommand{\paragraph}[1]{\bigskip\par\noindent\textbf{#1}~---}
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\sectionname}{Section}
\newcommand{\secref}[1]{\sectionname~\ref{#1}}
\newcommand{\footnoteref}[1]{note~\ref{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}

% math macros
\newcommand{\e}{\mathrm{e}} % why do I have to define this?
\newcommand{\dd}{\mathrm{d}}
\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\norm}[1]{\lVert{#1}\rVert}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\title{\bfseries
What is a measurement?}
\author{David W. Hogg\footnote{%
DWH (david.hogg@nyu.edu) is in the Center for Cosmology and Particle Physics, Department of Physics, New York University.
He also has appointments at the Center for Computational Astrophysics, Flatiron Institute, and at the Max-Planck-Institut f\"ur Astronomie.}}
\date{2025 August}
\begin{document}
\maketitle\thispagestyle{empty}

\paragraph{Abstract}
A measurement of a parameter $\theta$ is a quantitative constraint flowing from some particular data $Y$; for example, the data from the ESA \textsl{Planck} Mission measured the matter density of the Universe.
In both Bayesian and frequentist practice, all the evidence or information in a data sample about the parameters of a model is contained in the likelihood function (LF); the LF is sufficient.
Thus if parameter $\theta$ is measured with data $Y$, there must be a neighborhood in $\theta$ in which the LF has a (sufficiently) tall peak.
Posterior pdfs, machine-learning regressions, and marginalized likelihoods (marginalized over nuisances), can all deliver things that look like measurements; these are not measurements, unless (in addition) the LF shows a peak.
Construction of the LF involves making subjective choices and assumptions; in order for a measurement to be accepted by the community, these choices must be made in accordance with community norms.
I discuss how to demonstrate that the data $Y$ do (or can) provide a measurement of parameter $\theta$ even in the presence of substantial and covariant nuisances; it involves either profiling (optimizing over nuisances) or else taking the inverse of the Fisher information tensor.
I discuss investigator options when there is no LF, or when the LF is not tractable or not computable.

\section{Introduction}\label{sec:intro}
In the arguments about Bayesian and frequentist statistics, there are claims about subjectivity.
For example, frequentists (rightly) note that the introduction of a prior into an inference generally involves informative, subjective inputs.
For another, Bayesians (rightly) note that the calculus of probability is provably optimal for making certain kinds of probabilistic predictions or bets \cite{jaynes}; anything else involves subjective choices, at the expense of optimality.
It is increasingly recognized (despite monikers such as ``objective Bayes'' \cite{objective}) that no method for statistical inference is \emph{objectively correct}:
Every method for making a measurement, deciding among hypotheses, claiming a discovery from data, or predicting the outcome of a new experiment, involves myriad investigator choices.

This (purely pedagogical) \documentname{} is partly about claims of measurement:
When is it true that a data set $Y$ has delivered a measurement of parameter $\theta$?
My answer relates to the \emph{likelihood principle}, which I discuss in \secref{sec:lf}.
This principle---which applies in both Bayesian and frequentist statistical practice---is that all information in any data set $Y$ about any parameter $\theta$ is summarized completely by the likelihood function $p(Y\given\theta, I)$.
The likelihood function is a probability density for the data $Y$ given the parameter $\theta$ and possibly other relevant information or assumptions $I$.
The main point of this \documentname{} is that a claim of a measurement must be made---or be possible to make---using the likelihood function alone.
And, additionally, that the likelihood function must be based on assumptions or choices that represent consensus choices for the relevant scientific community.
If the claimed measurement can't be seen as a peak in a reasonably constructed likelihood function, then the claim is not (completely) substantiated by the data.
For example, the ESA \textsl{Planck} Mission \cite{Planck} delivered a measurement of the matter density of the Universe \cite{planck18};
this means that there is a tall, narrow peak in the \textsl{Planck} likelihood function in the cosmological-parameter space, and that peak corresponds to a particular value (or set of values) of the matter density.\footnote{Oddly, the \textsl{Planck} results are all shown as projections of posterior pdfs, so it isn't perfectly obvious to an outsider that in the cited paper \cite{planck18} there is indeed a peak in the likelihood. There is, though.}

One project that understands the likelihood principle is the ESA \textsl{Gaia} Mission\footnote{%
The \textsl{Gaia} Mission acts very much like it understands the likelihood principle; its data releases include quantitative descriptions of likelihood functions \cite{gaialf}.
That said, there is no documentary evidence that I can find that the project made the decisions it made specifically because of the likelihood principle.
Its design decisions may have been intuitively made, as far as I can tell, even though they were made absolutely correctly.}
\cite{gaia},
which is measuring (among many other things) the parallaxes to more than a billion stars in the Milky Way.
The parallax is a geometric measure of the inverse of the distance to the star.
This Mission is trying to deliver valuable information to the astronomical community, members of which can make measurements of, or update their beliefs about, individual stars, about stellar clusters, and about the populations of stars, all using the \textsl{Gaia} data releases.
The quantities in the \textsl{Gaia} Data Archive \cite{gaiadata} that represent its parallax measurements are maximum-likelihood parallax measurements, and uncertainty estimates based on a Fisher-matrix-like analysis of uncertainty.
These quantities can be cleanly combined into a good approximation of likelihood function \cite{gaialf}.
That is, the \textsl{Gaia} Catalog parallax and parallax uncertainty for any star are parameters of a form for the probability of the Gaia data given that star's parallax.
This is clean!
It permits any user of the Catalog to update their beliefs about that star's distance, or their beliefs about any other astrophysical inferences that involve that star's distance.
If the \textsl{Gaia} Catalog contained mean or median-of-posterior estimates or any other kind of biased estimators, the data would have been far harder (or even impossible) to use.

One result that motivated this \documentname{} is a ``cosmology with one galaxy'' claim \cite{onegalaxy}.
This claim is that there is significant information about some of the cosmological parameters---and especially the mean matter density of the Universe---in the measurable properties of a single galaxy.
This claim might be correct, but the paper does not demonstrate it.
The paper is not consistent with the likelihood principle; it does not ground its claim in the likelihood or in any proxy for the likelihood.
Instead it uses a machine-learning regression to substantiate its results; the issues with this will be discussed in \secref{sec:ml}.

I opened by noting that data analyses involve subjective choices.
There is another kind of subjectivity in play:
What method you use depends on what \emph{you want to do} with your data analysis.
The primary use case I am going to focus on is the case in which
\textsl{(a)}~you have a data set, and
\textsl{(b)}~you have a parameter of interest,
and \textsl{(c)}~you want to know \emph{what that particular data set has to say} about the parameter of interest.
There are other use cases.
Sometimes you are making the measurement because you want to make a \emph{prediction} for a future experiment.
That is, you want to know what some future data set will say about your parameter of interest.
Sometimes you are trying to make a \emph{decision} and your decision will depend on the value of the parameter, plus other things about your utility function (your economic goals, say).
We will discuss these alternative objectives a tiny bit in \secref{sec:bayes}, but mainly we will concentrate on answering the question ``what does my data\footnote{%
One of the questions I get asked most about data is whether the word ``data'' is singular or plural. I don't have a strong position. However, in this document I will attempt to treat the word ``data'' as a mass noun, like ``grass'' or ``hair.''}
say about my parameter of interest?''

My field is astrophysics; most of the examples I give will be astrophysical.
Furthermore, the astrophysics community (now) is very Bayesian.
For this reason, the document that follows will seem to many of my astrophysics colleagues as oddly frequentist.
It isn't!
The likelihood principle is a Bayesian principle:
The likelihood function is sufficient to deliver all information about the parameters coming from the data.
This is the key idea of Bayes's rule:
The prior is updated to the posterior with---and only with---the likelihood function.
Thus the question \emph{Do these data $Y$ deliver a measurement of parameter $\theta$?} must be answerable with a likelihood function---something like $p(Y\given\theta,I)$---if that likelihood function can be written down.
When the likelihood function cannot be written down, life is harder; we will address that case in \secref{sec:lfi}.

\section{The likelihood function}\label{sec:lf}
The problem setup will be that there is an ordered list of $n$ data elements $Y$ (``the data''), an ordered list of $m$ parameters of interest $\theta$, an ordered list of $s$ nuisance parameters $\alpha$, and a large set of choices, assumptions, and background information $I$.
In contexts like this, there are many meanings of the word ``model,'' but for me this will be something that can be used to compute probabilities in the space of the data.
Under some set $I$ of detailed assumptions, it is possible to construct a probability density function (or sometimes just ``pdf'' or ``probability'' in what follows) for the data $Y$
\begin{align}
    p(Y\given\theta,\alpha,I) ~;
\end{align}
this pdf\footnote{%
I apologize for this notation; it over-uses the letter ``$p$''.
This notation uses the arguments of $p()$ that are on the left side of the vertical bar to tell you the variables over which $p()$ delivers a probability density function.
A better notation is the statistics notation, which would be something like $p_y(Y\given\theta,\alpha,I)$, which makes clear that $p()$ is a pdf for $y$, evaluated at $Y$.
However, it is conventional in the physical sciences to use this over-loaded $p()$ notation.
I will adopt it, with reservations, everywhere.}
is called \emph{the likelihood function} (or sometimes just ``likelihood'') and it will depend (strongly, we very much hope) on the values of the parameters $(\theta, \alpha)$.
We hope that the likelihood depends strongly on the parameters because if it doesn't, the data won't give us a measurement.
One key idea here is that everything is always conditioned on the set $I$ of assumptions: There is no objective truth; there are only reasonable assumptions.
Another key idea here is that the combination of parameters $(\theta,\alpha)$ is comprehensive: Once you specify those, you have fully specified the relevant pdf.

You can often (but not always) think of the likelihood function as a prediction for the expectation of the data plus a noise model.
For example, if you believe that it is the expectation for the data that depends on the parameters $(\theta, \alpha)$, and (in addition) there is additive, zero-mean Gaussian noise, the likelihood function becomes
\begin{align}
    p(Y\given\theta,\alpha,I) &= \frac{1}{\norm{2\pi\,C}^{1/2}}\,\exp\left(-\frac{1}{2}\,[Y - \mu]^\top\,C^{-1}\,[Y - \mu]\right) ~,
\end{align}
where $\mu$ is the expectation for the data $Y$,
$C$ is the $n\times n$ variance tensor (covariance matrix) for the Gaussian noise,
and $\mu$ and $C$ both depend (or can depend) on the parameters $(\theta,\alpha)$.
This kind of form emphasizes that the likelihood function both tells you how you expect the data to change as you change the parameters, \emph{and} how precisely the data can be predicted, given the parameters.

I alluded to the likelihood principle in \secref{sec:intro}.
The principle is that everything about the data $Y$ that can possibly matter to the parameters $(\theta,\alpha)$ is contained in the likelihood function.
That is, if you have beliefs represented (accurately) by a prior pdf $p(\theta,\alpha\given I)$ for the parameters $(\theta,\alpha)$,
and then you get data $Y$,
and then you want to update your beliefs to a posterior pdf $p(\theta,\alpha\given Y,I)$, the only legitimate procedure for updating your beliefs is to multiply by the likelihood function $p(Y\given\theta,\alpha,I)$:
\begin{align}
    p(\theta,\alpha\given Y,I) &= \frac{1}{Z}\,p(\theta,\alpha\given I)\,p(Y\given\theta,\alpha,I) ~,\label{eq:bayes}
\end{align}
where $Z$ is a normalization that ensures that $\int p(\theta,\alpha\given Y,I)\,\dd\theta\,\dd\alpha=1$.
This procedure \eqref{eq:bayes} is called ``Bayes rule'' and is the basis of Bayesian reasoning.
This is the \emph{only way} that the data are permitted to be used to update your beliefs, in Bayesian reasoning.
That's the likelihood principle.

In many contexts, both conceptually and for reasons of numerical precision, it is better to work with the logarithm $\mathscr{L}$ of the likelihood function;
\begin{align}
    \mathscr{L}(\theta,\alpha) &= \ln p(Y\given\theta,\alpha,I) ~.
\end{align}
When we take the log---and we always use natural logarithms here---we usually explicitly write the log likelihood in terms of the parameters only.
The data are fixed, after all.

Something like the likelihood principle has a role in frequentist statistics too.
The principle in frequentism is that everything about the data $Y$ that is relevant to the parameters $(\theta,\alpha)$ is encoded in the likelihood function.
That is, the likelihood function is a sufficient statistic for the data, when your goals relate (only) to the estimation of the parameters $(\theta,\alpha)$.
Your best estimate $\hat{\theta}$ for the parameter $\theta$, and your uncertainty $\sigma_\theta$ on that parameter can all be derived from the likelihood function alone, for particular definitions of the word ``best.''\footnote{%
The definition of ``best'' estimator is out of scope for this document, but it relates to having minimum variance while also being asymptotically unbiased.}
The most important of these estimates is the maximum-likelihood estimate, found by maximizing the likelihood with respect to the parameters $\theta$ (and all the nuisance parameters $\alpha$).
\begin{align}
    \hat{\theta},\hat{\alpha} &= \argmax_{\theta,\alpha} \mathscr{L}(\theta,\alpha) ~.\label{eq:ML}
\end{align}
Whatever estimate of $\theta$ you want to make, and independently of whether you (or your audience) is frequentist or Bayesian, you haven't measured your parameter $\theta$ with your data $Y$ if there isn't a peak in your likelihood function at some well-defined value of $\theta$.

There is a very special case that arises when the model is linear,
such that the expectation for the data $Y$ can be written as $X\,a$ for some $n\times (m+s)$ rectangular design matrix $X$,
and when the noise is Gaussian with zero mean and known, fixed covariance tensor $C$.
Here $a = [\theta,\alpha]^\top$ is the $(m+s)$-dimensional concatenation of the $m$ parameters of interest and the $s$ nuisances.
In this case, because the logarithm of a Gaussian is quadratic, there is a closed form for the maximum-likelihood parameters $\hat\theta$
\begin{align}
    \hat a &= [X^\top\,C^{-1}\,X]^{-1}\,X^\top\,C^{-1}\,Y \label{eq:wls} \\
    \begin{bmatrix}\hat\theta \\ \hat\alpha\end{bmatrix} &= \hat a ~.
\end{align}
This form will reappear in examples below.
The formal uncertainty on $\hat\theta$ is given by an $m\times m$ submatrix $C_\theta$ of the full $(m+s)\times (m+s)$ covariance matrix $C_a$, the inverse of which is simply a projection of the inverse covariance matrix $C^{-1}$ for the data $Y$
\begin{align}
    C_a^{-1} &= X^\top\,C^{-1}\,X \label{eq:Ca} \\
    \begin{bmatrix}C_\theta & C_{\theta\alpha} \\ C_{\theta\alpha}^\top & C_\alpha\end{bmatrix} &= C_a ~.
\end{align}
Very importantly for taking care of covariant nuisance parameters:
If you want the correctly profiled uncertainties (or correctly marginalized uncertainties) $C_\theta$ on the parameters of interest $\theta$, profiling (or marginalizing) out the nuisance parameters $\alpha$, you must invert $C_a^{-1}$ to get $C_a$ prior to breaking out the $m\times m$ submatrix $C_\theta$.
If $C_a^{-1}$ is not invertible, you don't have a measurement (see \secref{sec:flat}).

This expression \eqref{eq:Ca} is the linear algebra (or multivariate) form of what is called in undergraduate physics laboratory classes, ``error propagation.''
It is interesting to think about the error propagation $\eqref{eq:Ca}$ in terms of matrix shapes and units.
The design matrix $X$ can be seen as a translator from parameter space to data space, and since $C^{-1}$ has units of inverse data squared, the whole thing works.
For more information about this kind of problem, see the first sections of \cite{fittingaline}.

There are two very important properties of the likelihood function, when it comes to measurement.
The first is that differences in the natural logarithm of the likelihood have an absolute meaning:
A difference of 0.5 in natural log (a likelihood ratio of $\sqrt{\e}$) between two models is something akin to ``one sigma'' and a difference of 12.5 is something akin to ``five sigma.''\footnote{%
This kind of relationship---between log-likelihood differences and numbers of standard deviations equivalent---depends on the property that the models being compared are ``nested.''
For more information, see any classic statistics textbook, for example \cite{casellaberger}.}
This (it turns out) means that the second derivative of the log likelihood function with respect to parameters is related to measurement uncertainty.
This is formalized in the Cram\'er--Rao bound \cite{cramer, rao} on the variance (and bias) of estimators; more on this in \secref{sec:inference}.
It also means that any claim of a measurement requires a large difference in log likelihood between a model with a best-fit value of some parameter $\theta$ and a null model in which that parameter vanishes.

That is---and critical to everything that follows in this \documentname---any claim of a measurement of any parameter $\theta$ in data $Y$ requires not just a peak in the likelihood at some value of the parameter $\theta$, but a tall peak.
The peak in the log likelihood must be taller than its surroundings by log-likelihood differences much larger than unity.\footnote{Here, and everywhere in this document, the words ``log'' or ``logarithm'' or ``logarithmic'' always refer to natural logarithms in base $\e$.}
In particular, the log likelihood difference between the measured value of $\theta$ and the values that the investigator or project claims to ``rule out'' with their data must be (substantially) larger than unity.

The second important property of the likelihood function is that it is par\-am\-e\-ter\-i\-za\-tion-invariant.
If the problem is reparameterized from parameters $(\theta,\alpha)$ to new parameters $\beta$ such that the transformation between $(\theta,\alpha)$ and $\beta$ is invertible (and the inverse is also invertible), then the value of the likelihood at $(\theta,\alpha)$ will be precisely equal to the likelihood at the corresponding value of $\beta$.
This invariance is extremely important to frequentist statistics, and does not hold for Bayesian posteriors.
The fact that Bayesian priors and posteriors do not have reparameterization invariance will come back up in \secref{sec:bayes} and \secref{sec:flat}.

\section{Nuisance parameters}\label{sec:nuisance}
The nuisance parameters $\alpha$ substantially complicate questions of measurement when one is only interested in the parameters of interest $\theta$.
Many of the mistakes in the literature are mistakes in the handling of nuisance parameters.
This is in part because it is hard to get rid of the nuisance parameters without accidentally introducing information about the parameter of interest.
Any information brought in that doesn't come directly from the data $Y$---for example any information in a prior pdf over the nuisances---is information that can deceive an investigator into mistakenly claiming a measurement or over-estimating the information in the data $Y$ about the parameter of interest $\theta$.

How do we account for the nuisances $\alpha$ when we are claiming a measurement of the parameter of interest $\theta$?
For the committed Bayesians, there is only one answer, and that is to marginalize.
That is, you put a prior $p(\alpha\given I)$ on the nuisance parameters and integrate
\begin{align}
    p(\theta\given Y,I) &= \int p(\theta,\alpha\given Y,I)\,p(\alpha\given I)\,\dd\alpha ~,\label{eq:marginallf}
\end{align}
where the integral over $\alpha$ is, implicitly the integral over all of the nuisance parameter space, or the sum over all possible settings of $\alpha$ if $\alpha$ is discrete.
You might be tempted to conclude that we have a measurement of $\theta$ if there is a peak in this marginal likelihood.
That will turn out to be wrong, as we will show in \secref{sec:flat}, and related to ideas that come up in \secref{sec:ml}:
If the data contain lots of information about the nuisances $\alpha$ and the highest-likelihood values of the nuisances depend on the parameter of interest $\theta$, then these kinds of marginalizations will in general produce peaks in the marginal likelihood even when there is no information about $\theta$ in the data $Y$.
That's true even when those priors on the nuisances are flat, as we explicitly show in \secref{sec:flat}.

On the subject of nuisance parameters,
there is an extremely important idea in frequentist statistics, which is that of the \emph{profile likelihood}.
\begin{align}
    \mathscr{L}_\alpha(\theta) &= \max_\alpha \mathscr{L}(\theta,\alpha) ~,
\end{align}
where the subscript reminds us what we've profiled over, and the max operation means the following:
For every value of $\theta$, deliver the log likelihood for the maximum-likelihood value of $\alpha$ at that value of $\theta$.
This answers the questions of the form:
Is there \emph{any} setting of $\alpha$ that makes the data probable for this value of $\theta$?
That is, is it likely to make the data at this value of $\theta$ (when I have no opinions about $\alpha$)?
Bayesians can make a marginalized likelihood if they want to get rid of $\alpha$ (provided that they have a prior pdf on $\alpha$).
Frequentists don't usually have a pdf to apply on any parameters, so they can't marginalize\footnote{%
There are exceptions to this; sometimes frequentists \emph{can} marginalize.
One example in particle physics is that, in collisions, certain aspects of phase-space distributions, like jet angles, have known distributions.
The fact that the jet angles will be drawn from a particular distribuition is \emph{part of the model}, not an external prior.
Thus even a frequentist can marginalize over these angles without bringing in any additional information.
In this case, the jet angles can be seen as a kind of noise.
There are many places in physics and astronomy in which there are exact, compact symmetries, like rotational symmetries, that permit exact marginalizations of nuisances by both frequentists and Bayesians.
These ideas are related to the core ideas of (poorly named) \emph{objective Bayesian analysis} \cite{objective}.};
they have to optimize rather than marginalize over alpha.
Frequentists therefore must construct a profile likelihood if they want to get rid of $\alpha$.\footnote{%
I like to joke that Bayesians integrate, and frequentists differentiate. It isn't entirely a joke.\label{foot:integrate}}

Sometimes Bayesians do a really big integral, which is the likelihood marginalized over \emph{all} parameters,
and call it the \emph{evidence} or the \emph{fully marginalized likelihood}.
This is not relevant to our goals here, so I will leave my criticisms for a different rant.

There are rare cases in which you really do have quantitatively accurate beliefs about nuisance parameters, and thus can do the marginalization integral \eqref{eq:marginallf} safely.
One case is that in which your beliefs about the nuisance parameters themselves come from a prior experimental project constraining the nuisance parameters (a project executed with relatively uninformative priors).
This situation arises, for example, when the nuisance parameters have been constrained by a calibration program, and the distribution $p(\alpha)$ is really the posterior pdf from that calibration program.
Then this marginalization \eqref{eq:marginallf} is justifiable, although, instead, you could have just added the calibration-program data into your data set $Y$ and done everything simultaneously, and avoided marginalizations.
For many reasons, however, marginalization with a prior on $\alpha$ is often the only viable approach.

Another related case in which you can marginalize is when your scientific community really does have a widely agreed-upon prior pdf for your nuisances $\alpha$.
This case has come up recently in cosmology, where the posteriors (or likelihood functions) from the final analyses of the ESA \textsl{Planck} Mission \cite{planck18} are considered reasonable priors for the cosmological parameters in subsequent analyses.
This example demonstrates the social-construction aspect of measurement:
A prior taken from \textsl{Planck} is acceptable not because it is \emph{correct} but because \emph{the community accepts it}; it is consistent with the \emph{beliefs} of the community (and Bayesian inferences are about quantitatively updating beliefs).

\section{Statistical inference}\label{sec:inference}
Fundamentally, \emph{inference} is the estimation or measurement or prediction of $\theta$ given data $Y$.
For a frequentist, this involves an \emph{estimator}, which could be maximum-likelihood or otherwise.
For a Bayesian, this means making a likelihood function, and (perhaps) multiplying it by a prior and renormalizing to a posterior pdf (or sampling with MCMC \cite{mcmc}).
In what follows, for specificity, we will assume that the frequentists use maximum-likelihood estimators.
Bayesians don't necessarily have estimators, but sometimes a Bayesian will report a posterior mean or median or (wrongly\footnote{%
Again, Bayesians integrate and frequentists differentiate. Bayesian beliefs are much better summarized with means or medians of posteriors than optima of posteriors: The optimum of the posterior can be far from the bulk of the posterior density, oddly.})
a maximum of posterior.

Sometimes Bayesians like to think that, since they have a framework of prior pdf and posterior pdf, they don't need to ever choose an estimator.
There is an element of truth to this!
However even the Bayesians have to decide what number (or numbers or intervals) they \emph{report in the abstract of their paper} or in their summary slides about their parameter of interest $\theta$ (I say more about this in \ref{sec:reporting}).
Thus even Bayesians have estimators at some level.
The qualities of estimators---in terms of bias and variance---are limited by a beautiful set of mathematics in the area of \emph{information theory}.
The most important, relevant result is the Cram\'er--Rao bound \cite{cramer, rao}, which limits the variance estimators.
This bound says that, given data $Y$, the best an unbiased estimator can do is set by the Fisher Information \cite{fisher},
One consequence of all this is that any claim about the ability of some data set $Y$ to measure or constrain some parameter or parameters of interest $\theta$ in the presence of nuisances $\alpha$ can all be boiled down to second derivatives of the log-likelihood $\mathscr{L}(\theta,\alpha)$.

Getting more specific:
It is possible to think about whether data $Y$ have provided a measurement of parameter $\theta$ by thinking about the \emph{uncertainty}\footnote{%
It's not an error bar or an error, it's an uncertainty. If it were an error, we would correct it.}
in the measurement of $\theta$.
Cram\'er--Rao guides us here:
This bound says that the best you can possibly do, in an unbiased-estimator sense, on your parameter of interest, given your data $Y$, is the square root of the inverse of the Fisher information.
That is, you start by computing the information tensor $C^{-1}_{(\theta,\alpha)}$
\begin{align}
    C^{-1}_{(\theta,\alpha)} &= - \frac{\dd^2\mathscr{L}}{\dd(\theta,\alpha)^2} ~.\label{eq:fisher}
\end{align}
This second-derivative tensor is $(m+s)\times (m+s)$, where $(m+s)$ is the total number of elements in the full set of parameters $a = [\theta,\alpha]^\top$.
Then you invert this matrix to get $C_{(\theta,\alpha)}$.
Then you take the (square) $m\times m$ submatrix of $C_{(\theta,\alpha)}$ corresponding to the $m$ parameters in $\theta$ only.\footnote{It is critical here that you invert before you take the submatrix. If you submatrix before you invert, you will get disastrously wrong uncertainty estimates. In that (wrong) case you have computed the best possible uncertainty under the (wrong) assumption that God told you the correct values of the nuisance parameters.}
If there is only one, scalar parameter of interest $\theta$, then this gives the uncertainty variance (the square of the uncertainty) on $\theta$.
If $m>1$---that is, if $\theta$ has more than one component---this $m\times m$ submatrix is the variance tensor of the uncertainty ellipse in the $\theta$ space.
Notice that this is all a generalization of the discussion of linear fitting above in \secref{sec:lf} around and after \eqref{eq:Ca}.
HOGG DO WE NEED SOME PICTURES OR DIAGRAMS?
Importantly, this bound sets the properties of any possible estimator.
If this Cram\'er--Rao (or Fisher-matrix) uncertainty is larger than what you need to claim a (say 5-sigma) measurement, then \emph{there is simply no sense in which data $Y$ have measured parameter $\theta$}.

Second derivatives like in \eqref{eq:fisher} sound hard!
But they aren't, for two reasons.
The first is that, when your model is linear, and your noise is Gaussian, this second derivative becomes an outer product of first derivatives (see, for example, \cite{fittingaline}).
The second is that we live in the future, in which programming languages like jax \cite{jax} give you analytic derivatives to second order for free.

Frequentists and Bayesians are all bound by Cram\'er--Rao.
However, Bayesians can do more than frequentists.
Bayesians can update beliefs, compute optimal betting odds on future outcomes, and optimize utility over decisions, integrating over all possible states of the world.
All these things are important, but they don't change the fundamental point that the likelihood function is sufficient for conveying the information in data $Y$ about parameter $\theta$ and thus the likelihood function is the only thing in play when a claim of measurement is being made.

\section{Isn't Bayes The Best Thing To Do (tm)?}\label{sec:bayes}
The most beautiful thing about Bayesian inference is that it is the provably correct way for an individual to reason about uncertain things.\footnote{%
The theorems are known as the Cox theorems.
The best explanation of them I know is the first Chapter of Jaynes's book \cite{jaynes}.}
The rules of probability theory are also the rules about reasoning about the plausibility of the things you know, or want to know.
For this reason, it makes sense for a scientist---or a human being---to reason in a Bayesian manner.
I would even say (and I have said elsewhere \cite{plausibility})
that, for the reason of this provable correctness, the whole scientific community ought to act, collectively, in a kind of Bayesian way.
It should have a collective set of beliefs and it should update those collective beliefs by (at least approximately) multiplying by a likelihood function for the new data it obtains each year.

Does this mean that \emph{every scientific paper} should describe a prior pdf, a likelihood, and a posterior obtained by Bayes's rule?
No, it does not.
The point of each scientific paper is \emph{not}---or not mainly---to describe to the community \emph{the authors' beliefs}.
The point of each scientific paper is to move forward the beliefs of the community of scientists.
That moving forward could be contributions to, or arguments about, the community's prior beliefs, the community's many likelihood functions, and the community's posterior beliefs.
Each paper has a small role in this program; it is not the case that each paper executes the whole program for the whole community.\footnote{%
There are many treatises on the philosophy of science.
I feel like we need a philosophical theory of the \emph{scientific paper} or of the \emph{individual scientific contribution}.}

When scientist A reads a paper written by scientist B, what does scientist A hope to get from that paper, and what did scientist B hope to provide in that paper?
I believe that scientist A wants to know if their own (scientist A's own) beliefs need to be updated.
I believe that scientist B wants to influence the beliefs of scientist A.
In either case, what scientist A needs to get from the paper is not scientist B's posterior pdf.
What scientist A needs to get from the paper is scientist B's likelihood function (or, alternatively, scientist B's data; we will return to that idea below in \secref{sec:catalogs}).

If scientist A can understand clearly the assumptions going into scientist B's likelihood function,
and if scientist A broadly agrees with them,
then scientist A can simply update their beliefs by multiplying (approximately or as exactly as possible, depending on the nature of the work)
their own (scientist A's own) prior pdf with scientist B's likelihood function.
And it is worthy of note that scientist A is much more prone to agree with the assumptions going into scientist B's likelihood function if scientist B's assumptions are aligned with community norms in the field.

When I say that scientist A will multiply their prior pdf by scientist B's likelihood function,
I don't necessarily mean that they will do so by writing a piece of code that imports or reimplements prior pdfs, imports the likelihood function, and literally does the multiply and representation of the output.
I mean something more approximate and vague.
But sometimes it \emph{literally is the case} that this code and import and multiply \emph{is} what scientist A does.
In the case from \secref{sec:intro} with the ESA \textsl{Gaia} Catalog, that is exactly what we have been doing \cite{gaialf}.
And NASA created the \textsl{LAMBDA} archive in part to preserve executable likelihood functions in cosmology for exactly this purpose \cite{lambda}.

Of course this is a dream.
In many real cases, scientist A will not agree with the assumptions going into scientist B's likelihood function.
But usually scientist A will agree with a lot of those assumptions.
Then scientist B's paper is a template for making a new likelihood function with modified assumptions, which scientist A can then use to modify scientist A's beliefs.
This is a less direct use of scientist B's likelihood function, but still very useful.

The point of scientist B's paper is not to represent scientist B's beliefs, but instead to aid scientist A in \emph{updating} scientist A's beliefs, or maybe the beliefs of some community of which scientist A is a part.
Beliefs of individual scientists (or even whole scientific communities) are idiosyncratic and subjective.
If we want to update someone \emph{else's} beliefs, we must provide them with a likelihood function.
The point of (most of) the scientific literature is to communicate about (aspects of) likelihood functions.
When a paper multiplies a prior by a likelihood to produce a posterior it is---at best---demonstrating the effect of the data on typical beliefs.
At worst it is irrelevant to every individual reader.\footnote{%
There is only one important sense in which such a paper---a paper that takes one investigator's subjective prior beliefs and multiplies them by one data set's likelihood function to deliver the updated investigator's subjective posterior beliefs---is a useful scientific product.
It is that it demonstrates the procedure of constructing a prior pdf, constructing a sensible (we hope) likelihood function, and combining them into a posterior pdf.
That demonstration can be useful to others, especially if the methods are unambiguously explained and the codes are released for re-use.
But the specific value or shape of the individual investigator's particular posterior pdf is not an object of community scientific interest, unless the author is an \emph{extremely important person}.}

HOGG: HERE is where we would talk about LIGO posterior samplings and LIGO populations inferences.
And the point / result that if the number of sources gets large, current methods don't work!

This \sectionname{} opened with the point that Bayesian reasoning is optimal.
In what very specific sense is it optimal?
It is best for \emph{placing bets};
One statement of the theorems is that if two bettors have access to the same information, and one reasons via Bayes, and one reasons in any other way, the Bayesian bettor will win bets (in the long run) against the other.
Thus Bayes is optimal for making predictions, and in particular putting betting odds on predictions.
This explains why Bayesian reasoning was developed by the insurance industry \cite{bayesactuary}.
Science involves prediction, crucially.
However, measurements are not the same as predictions.
They can be similar though: A measurement can (in principle) be seen as a prediction for what a subsequent experiment might measure.

There is another use case for Bayesian reasoning, which is closely related, and it is decision-making:
If you have to make a decision (which side of a bet to take, which model to prefer, what to have for lunch, and so on),
and if you can specify your \emph{utility function}, you can use Bayesian posteriors to compute expectations of utility.
This is out of scope for this document, but briefly your utility is the amount of money (or money equivalent) that you make or lose in different future outcomes, as a function of your decision.
Again it makes sense that the insurance industry would care.
But again, measurements are not the same as decisions.\footnote{%
It might seem obvious that measurements are not decisions!
But in fact any paper about any measurement says things in its abstract (and body) about the value obtained by the measurement process, and the assumptions that went into the measurement, and what should or could be concluded from that measurement.
Those are choices, which can be phrased as decisions.
So even if we are reporting a measurement based only on the likelihood function, perhaps we should be Bayesian when we think about what words to write where and in what order?}

The theorems about Bayesian reasoning are correct of course (they're theorems!).
You, personally, ought to reason according to Bayes (for example when you are deciding what to order at a restaurant, negotiating salary, or deciding whether to buy insurance).
Similarly, the scientific community taken as a whole ought to update its beliefs (inasmuch as a community has beliefs) according to Bayes.
The important thing is that \emph{a measurement is not the same as a belief}.
Measurements are used to \emph{update} your beliefs.
Hence measurements are properties of your likelihood function, not properties of your posterior pdf.
That's almost the main point of this \documentname.

Finally, I remark that people often say to me: ``But Bayesian inference with flat priors is identical to maximum likelihood, so it doesn't matter.''
This is absolutely \emph{not true}, as I discuss in \secref{sec:flat}.
If you don't want priors to bias your measurement, \emph{don't use priors}.

\section{Can't I just use flat priors?}\label{sec:flat}
We need a peak in the likelihood function to claim a measurement.
But if we want to be Bayesian, and we want to use \emph{flat priors},\footnote{%
If you want to be Bayesian, there is almost no context in which you would want to use flat priors on any parameter you care about.
The Cox theorems that deliver Bayesian reasoning only apply when the Bayesian procedure is being used to update \emph{your beliefs}.
It is essentially impossible that your belief about anything is well represented by a flat pdf.}
isn't it the case that any peak in the likelihood will lead to an identically shaped peak in the posterior pdf?

This sounds so naively and obviously true, what could possibly go wrong?
There are two things that are very wrong about this idea---the idea that if you use flat priors its the same as having just a likelihood function.
The first thing that is wrong is that (as we emphasized in \secref{sec:lf}) the likelihood function is reparameterization-invariant.
The posterior pdf is absolutely not reparameterization-invariant.
So if the prior is flat in some parameter, it is absolutely not flat in any non-trivial function of that parameter.
Even worse, if it is flat in some parameter, in any square-root or square or logarithm of that parameter, the prior is strongly peaked \emph{at one edge of the allowed range!}
That's demonstrated in FIGURE HOGG and definitely not what people usually have in mind when they think about using ``uninformative priors.''

The second thing that is wrong about this idea is that when the number of parameters gets large---larger than 2 say---it becomes very easy to believe that there is a peak in the likelihood function when there most definitely isn't.
To demonstrate this, we generate a very simple toy-likelihood and toy-posterior situation, in which all priors are flat, and in which it very very much looks like there is a peak in the likelihood function when there absolutely is not.

In our toy problem we have data $Y$ that makes up a $n\times 1$ column vector, or an ordered list of $n$ elements.
We have parameters $\theta$, which make up a $m\times 1$ column vector, or an ordered list of $m$ elements.
If we use the symbol $\theta_i$ to denote the $i$th element ($1\leq i\leq m$) of this vector, we could say that $\theta_1$ is the parameter of interest that we care about and the other $\theta_i$ (for $i>1$) are the nuisance parameters $\alpha$ (HOGG AUDIT NOTATION) that we don't care about.
The simplest possible toy model is that the LF depends only on certain linear combinations of the parameters $\theta$.
That is, there is a rectangular $q\times m$ matrix $A$, with $q<m<n$, such that, to some very good approximation,
\begin{align}
    \mathscr{L}(\theta) \equiv \ln p(Y\given\theta,I) &= K - \frac{1}{2}\,\norm{A\cdot\theta - a}^2 ~,\label{eq:toyLF}
\end{align}
where $K$ is some scalar constant, $a$ is a $q$-vector constant, and the norm is the standard euclidean norm (square root of sum of squares).
This model might be called ``low rank'' or ``degenerate'':
There are parameter degeneracies such that the likelihood depends on less than all the parameters.
And, by the way, this toy example is extreme; we don't need these parameter degeneracies to be exact to get the effect we are about to see.

HOGG FIX THIS to represent the new situation, which is a standard linear LF, but with a design matrix that is LOW RANK; rank $q$ with $q<p<n$.

(HOGG DESCRIBE PRIOR by adjusting the following:)
When we do Bayesian inference, we will want a prior, and, because we want to be maximally uninformative, we will choose a prior that is precisely flat in all parameters $\theta_i$.
That is
\begin{align}
    p(\theta\given I) &= \prod_{i=1}^p U(\theta_i\given a_i, b_i) \\
    U(x\given a, b) & = \left\{\begin{array}{cl}
    0 & \mbox{for $x<a$,} \\
    (b-a)^{-1} & \mbox{for $a<x<b$,} \\
    0 & \mbox{for $b<x$,}\end{array}\right.
\end{align}
where the $a_i, b_i$ ($1\leq i\leq m$) are prior limit parameters with $a_i<b_i$ for all $i$.
In fact, these flat priors \emph{aren't} uninformative (CITE THINGS), but perhaps, in some circumstances, they are least informative?
It turns out that they will be quite informative in what follows.
This prior requires us to make $2\,m$ choices, to wit, the upper and lower limits $a_i, b_i$ for each parameter $\theta_i$.

HOGG SHOW RESULTS.

There is nothing really new about this demonstration, by the way.
They are all simple consequences of prior-volume effects or prior-edge effects discussed in many places (CITE THINGS).
This demonstration also links to the idea that flat priors aren't in fact uninformative, as has been argued elsewhere in many places (CITE THINGS).
HOGG: ALSO this situation is very easy to diagnose (significant prior-dependence of results, even when adjusting only the nuisance priors).

\section{What if my likelihood has multiple peaks?}\label{sec:multiple}
HOGG: What?

\section{Machine learning or regression}\label{sec:ml}
There are many ways in which machine learning might enter into a project to measure a parameter $\theta$ given data $Y$.
Here we will only consider one, which is \emph{supervised regression}:
The setup for supervised regression is that there is a \emph{training set} $\setof{Y_i, \theta_i}_{i=1}^n$ of $n$ data instances $Y_i$ and corresponding labels $\theta_i$.
We are going to \emph{learn} a flexible function $f(Y;W)$, parameterized by a large block of weights $W$, that does a good job of predicting labels $\theta_i$ for training-set examples $Y_i$.
Here $f(Y;W)$ is the multi-layer perceptron or transformer or whatever highly flexible machine-learning model and $W$ represents all the biases and weights inside that model.
We will estimate our parameter of interest $\theta$ by executing the learned function on our data $Y$.
In equations, this looks something like
\begin{align}
    \hat{W} &= \argmin_W \sum_i\norm{\theta_i - f(Y_i;W)} \\
    \hat{\theta} &= f(Y;\hat{W}) ~,
\end{align}
where $\norm{x}$ is some kind of metric distance (like the Euclidean norm or the square of that).

Ideally the labels $\theta_i$ in your training set are very reliable and accurate estimates for each training data object $Y_i$.
If they are noisy, pure regressions aren't a good idea; it makes more sense to go to models with a generative aspect (for example, CITE).
Ideally the data $Y_i$ in your training set are somehow ``drawn'' from the same distribution as the data $Y$ you have.
If they are not drawn from the same distribution, strong biases will occur.
Often it is believed that so long as the training data ``cover'' the test data (for example the test data live inside the convex hull of the training data), then things are fine.
This isn't true, and besides, once the data get large enough, coverage of that kind isn't possible to satisfy.

It turns out that, even when the situation is ideal---%
even when the training labels are perfect, the data are drawn from the same distribution as the training data, and the function $f()$ has been given sufficient flexibility---the results of machine learning regressions are biased.
Full analysis of this is out of scope; it is discussed extensively elsewhere (for example, \cite{goodorbad, ting}).
It relates to the point made above in \secref{sec:bayes}:
Good predictions (like those produced by Bayesian inferences) are biased, and machine learning regressions are optimized to make good predictions.
The connections between machine learning regressions and Bayesian inferences are strong:
Machine learning regressions produce results with a lot of properties similar to maximum \foreign{a posteriori} (maximum-of-posterior) parameter estimates, with the training data serving as an implicit prior.

Is a machine learning regression output a measurement?
No, it is not, or at least not in itself:
When a machine learning method returns a result, it is using information from your data $Y$
but also from all of the training data $\setof{Y_i, \theta_i}_{i=1}^n$.
That is, it cannot be described as telling you what, in your data $Y$, you have learned about parameter $\theta$.

Another way to see this is:
A regression, fundamentally, is a kind of nearest-neighbor method.
Different regression methods are only different in their sophistication about finding neighbors.
The fundamental assumption underlying a regression is that if two data sets $Y$ and $Y_i$ are ``similar'' in some important way, then their labels $\theta$ and $\theta_i$ must also be similar in some sense.
This might be true!
But it can be true for two reasons.
It can be true because the data $Y$ really contains information about the label $\theta$.
Or it can be true because objects that are similar in one way are similar in many other ways.
That is, the fact that a regression to determine label $\theta$ given data input $Y$ works well does not, in itself, imply that there is information about $\theta$ in $Y$.

To be concrete: Imagine that data $Y$ are very informative about some quantity or label $\gamma$.
And imagine that, in the training set, objects with high $\theta$ also tend to have high $\gamma$, and \foreign{vice versa}.
Then a regression trained on the training set will deliver good predictions of labels $\theta$, \emph{whether or not} the data $Y$ contains information about $\theta$ directly.
The regression does not return measurements of $\theta$.
It returns \emph{predictions}, under the assumption that the training set is representative.

\section{What is my data?}\label{sec:data}
Ideally, you want to be operating on data $Y$ that is as raw as possible, or as close to raw as possible.
That is, it is better to work on the data that came directly from your instrument, and not data that have been highly processed, especially since that processing can significantly bias or affect your results.

Of course, most data are highly processed: Raw instrument read-outs are generally corrected for instrumental biases and spurious signals, and calibrated to interpretable units.
These corrections are probably necessary for making good measurements!
That said, there is no reason that these corrections need to be applied to the raw data before you work with it.
Any calibration or correction procedure that divides the raw data by $a$, say, and then subtracts $b$ could, instead, be obviated:
Instead, the investigator could add $b$ to the model---the prediction for the data---and then multiply the model by $a$, thus making the calibration something that is applied to the model, not the data.
That is, any calibration parameters that are used to correct the raw data to corrected data could instead be thought of as model parameters, that correct the idealized data prediction to a prediction for the raw data.
Whenever this is possible, I recommend it, because it unifies and makes explicit everything that goes into the data analysis.
Many times, however, for practical reasons, this is not possible, and the data $Y$ is instead derived by some process from some raw data $Z$.

There are three-ish scenarios in which the derived data $Y$ might have been made from the raw data $Z$:
\begin{enumerate}
\item In the first scenario, some aribitrary algorithm, not justified by any statistical theory, might have been applied to the raw data, such that $Y=g(Z)$ for some function $g()$.
The uncertainties on $Y$ in this scenario might be derived from the raw uncertainties on $Z$  by multiplying the raw uncertainties by (the absolute value of) the derivative $\dd g/\dd Z$, or perhaps something more sophisticated than that.
This is typically the scenario in astronomy when the raw image from a camera has an estimate of the dark (or bias) subtracted, and then some kind of flat-field divided out, then a gain estimate applied, and then something that calibrates the image to intensity units (say).
\item In the second scenario, the derived data might be, somehow, the maximum-likelihood value of $Y$ for some probabilistic model $\ln p(Z\given Y,I)$, under assumptions $I$.
The uncertainties on $Y$ in this scenario might be made from the second derivative of the log likelihood, according to information theory (the Fisher information, that is HOGG REFER TO SOMETHING ABOVE).
This is the scenario for the ESA \textsl{Gaia} Mission, which delivers maximum-likelihood parallaxes and proper motions for all stars, along with (an approximation to) the (inverse of the) Fisher information tensor from the log likelihood function that was used to measure those parallaxes (CITE).
\item In the third scenario, the derived data might be the mean or maximum of a posterior pdf for $Y$, generated using a log likelihood $\ln p(Z\given Y,I)$ and a log prior pdf $\ln p(Y\given I)$.
The uncertainties on $Y$ in this scenario are usually made by estimating the second moment (the variance) of the posterior pdf.
In astronomy, this is the scenario for exoplanet transit spectroscopy, where, in each wavelength bin, differences in spectral flux as a function of time are generally modeled with a Bayesian analysis using a probabilistic model for the transit.
\end{enumerate}
While all three of these scenarios can be workable for doing good data analysis, there are arbitrarily bad things that can happen in the first and third.
In the first, if the function $g(Z)$ is sufficiently nonlinear, the uncertainty estimates can become arbitrarily wrong, or arbitrarily difficult to make right.
It is far better to apply $g^{-1}()$ to your prediction for the derived data $Y$ and thus make a prediction for $Z$ directly, than it is to just predict $Y$.

In the third scenario, the Bayesian inference produces biased estimators, such that the data $Y$ can become prior-distorted or even effectively prior-dominated.
In this case, the data $Y$ dont tell you just about your raw data; they contain also information about your priors, which distorts the data, and also makes multiple data points correlated (since they will share aspects of this prior).
In general, the third scenario is to be avoided, because it is very hard to tell in the abstract how much the prior matters; indeed it will matter differently to different models and to different settings of your parameters $(\theta,\alpha)$.

The reason that the second scenario is to be preferred is because likelihood functions can be daisy-chained, in the following sense:
\begin{align}
    p(Z\given\theta,\alpha,I) &= \int p(Z\given Y,I)\,p(Y\given\theta,\alpha,I)\,\dd Y ~,\label{eq:ZtoY}
\end{align}
where the integral is (implicitly) over the full possible domain of $Y$.
That is, the raw-data likelihood can be very simply related to the derived-data likelihood, provided that the investigator has access to the intermediate likelihood function $p(Z\given Y,I)$ that relates the raw data $Z$ to the derived data $Y$ (or if the investigator is given enough information to reconstruct that function).
Because of the likelihood principle (\secref{sec:lf}), if we want to make measurements with the data, we want the data to be useful for making likelihood estimates or comparisons.
If the data $Y$ are related to the raw data $Z$ by a likelihood, then it is usually possible to perform a data analysis on data $Y$ but have it return exactly what \emph{would have been returned} if the analysis had been performed on the raw data $Z$.

All of this is very nicely demonstrated in terms of linear models with Gaussian noise.
Recall from \eqref{eq:wls} above that, in this case (linear, Gaussian), my maximum-likelihood estimate $\hat\theta$ is given by
\begin{align}
    \hat\theta &= [X^\top\,C^{-1}\,X]^{-1}\,X^\top\,C^{-1}\,Y ~,\label{eq:wls2}
\end{align}
for some $n\times m$ design matrix $X$ (dropping the nuisance parameters just for convenience here).
Imagine that the data $Y$ are developed from some truly raw data $Z$ by some linear process---for example, if the data $Y$ are the data $Z$ but binned down into coarser bins, say---and this binning is done using the uncertainties correctly; that is:
\begin{align}
    Y &= [B^\top\,C_Z^{-1}\,B]^{-1}\,B^\top\,C_Z^{-1}\,Z \label{eq:wls3} \\
    C^{-1} &= B^\top\,C_Z^{-1}\,B ~,
\end{align}
where $C_Z$ is the uncertainty variance tensor on the raw data $Z$.
Now if you combine \eqref{eq:wls2} and \eqref{eq:wls3} and use the relationship of $C^{-1}$ to $C_Z^{-1}$, you get
\begin{align}
    \hat\theta &= [(B\,X)^\top\,C_Z^{-1}\,(B\,X)]^{-1}\,(B\,X)^\top\,C_Z^{-1}\,Z ~.\label{eq:wls4}
\end{align}
That is, doing principled weighted least squares on the data $Y$ with design matrix $X$ is numerically identical to doing principled weighted least squares on the truly raw data $Z$ but with design matrix $(B\,X)$.

This is a strong argument for making sure that reduced data $Y$ relate to raw data $Z$ by simple linear expressions like $\eqref{eq:wls3}$.
Many data-reduction steps \emph{do} have this form or \emph{can} have this form.
I mentioned binning of the data: Provided that each bin of reduced data is the inverse-variance-weighted average of the raw data going into that bin, the binning is consistent with $\eqref{eq:wls3}$.
I mentioned flat-fielding and bias subtraction and so on in imaging projects:
Provided that these operations are linear and use inverse-variance weights on the raw data, these will also meet these conditions.
What I'm saying is: It isn't hard to work in ways that are consistent with these principles.
And it is essential in precise or rigorous contexts.

That said, any other transformation between raw data $Z$ and reduced data $Y$---that is, any transformation that uses anything other than a pure likelihood---will \emph{bias all results} made with the data $Y$.
For example, if the data $Y$ are found from any statistic (mean, median or mode) of a Bayesian posterior pdf, all scientific results made with the data $Y$ will be biased by the priors that went in.
The biases created by using posterior pdf instead of likelihood for going from $Z$ to $Y$ are not easy to know \foreign{a priori}, and they are generally impossible for downstream users to correct after the fact.
It might seem absurd that anyone would do this, but in fact this is very common in the astronomy literature.\footnote{HOGG CITE THINGS with tiny explanations.}
If you are reducing, calibrating, or binning data, use likelihood formulations or weighted least squares.
Don't use posterior inferences.

Similarly, if the data $Y$ is related to the raw data $Z$ by a regression.
That is, if the data $Y$ is a set of labels found by training a regression on a training set and then running it on the raw data $Z$, all scientific results made with the data $Y$ will be biased.
We described this in \secref{sec:ml} and it is discussed elsewhere \cite{goodorbad, ting}.

One final comment on this linear-fitting point is that, even obeying the very simple rule given in \eqref{eq:wls3} does not protect a project from harm.
Any binning or data reduction step in going from $Z$ to $Y$ can remove information.
For example, if the signals of interest in design matrix $X$ are high-frequency and the binning matrix $B$ has big bins, the binning will destroy that information.
That is, it is still better to work directly with the raw data $Z$ if you can.
And if the data reduction step looks as simple as \eqref{eq:wls3}, then you might as well just work with the raw data directly if you can get the raw data.
This is a realization that is adjacent to the phrase ``binning is sinning.''\footnote{This is a favorite phrase of my friend and mentor Hans-Walter Rix, and the title of a paper by Kipping \cite{binningissinning}. The Kipping paper is making precisely this point that binning destroys high-frequency information.}

When the model relating reduced data $Y$ to raw data $Z$ is nonlinear, it isn't possible to be absolutely principled in all this without explicitly doing the integral \eqref{eq:ZtoY}.
But when the problem is well-behaved or can be linearized near the optimum, using as the data $Y$ the maximum of the intermediate log-likelihood $\ln p(Y\given Z,I)$, and making the noise inverse-variance $C^{-1}$ the (negative of the) second derivative of that intermediate log-likelihood usually delivers a very good approximation to what you want.
In equations, this is
\begin{align}
    Y &= \argmax_{Y'} \ln p(Y'\given Z, I) \\
    C^{-1} &= - \left.\frac{\dd^2}{\dd {Y'}^2} \ln p(Y'\given Z, I)\right|_Y ~,
\end{align}
where I used a dummy variable $Y'$ to make the notation consistent.\footnote{Technically the second derivative should be an \emph{expectation over $Y$} and not just evaluated at $Y$; this matters when the second derivative is a very strong function of $Y'$.}
This is precisely what the \textsl{Gaia} Mission \cite{gaia} has done in delivering parallaxes and proper motions in their data release \cite{gaiadr3} (the reduced data $Y$) and their uncertainty variances $C$, which are all based on onboard timing of stellar transits (the raw data $Z$) and a timing noise model (parameterized by a $C_Z$).

When one works with derived data $Y$, derived from some data $Z$, one extremely technical final point is that it is essential that the data $Y$ be using the data $Z$ \emph{only once} in some sense.
That is, you can't make the data $Y$ be all the sums and differences of the data $Z$ without re-using data points from $Z$.
Well you \emph{can}, but then you have to explicitly track all of these covariances in $C$.
And really those covariances have to be tracked in $C^{-1}$ not $C$ because $C$ will contain infinite eigenvalues.
This is one of many reasons that we should be transmitting not variances but inverse variances in our datasets and data analyses.\footnote{The inverse variance is sometimes called the information tensor. Variances (and their eigenvalues) can go to infinity, but inverse variances are always finite in all respects, because one never has infinite amounts of information about anything.}

\section{Combining data samples}\label{sec:combining}
There are (at least) two cases for combining measurements.
In the first, there are two data sets, $Y_1$ and $Y_2$, and they both provide measurements of the parameters $\theta$ of interest, plus maybe nuisance parameters $\alpha_1$ and $\alpha_2$ (which in general will differ between $Y_1$ and $Y_2$).
An example from cosmology is the cosmological parameters $\theta$, which can be constrained with the power spectrum of the cosmic microwave background (which could be data $Y_1$) and correlation function of galaxies (which could be data $Y_2$); each of these two kinds of data have two different kinds of nuisance parameters, but they both contain information about $\theta$.

In this case, the goal might be to get the best possible measurement of the parameters $\theta$ given both data sets.
The great thing is---as long as the two data sets were created or made independently---the likelihood functions just multiply (the log likelihoods just add), such that
\begin{align}
    \mathscr{L}(\theta,\alpha_1,\alpha_2) &= \ln p(Y_1\given\theta,\alpha_1,I) + \ln p(Y_2\given\theta,\alpha_2,I) ~.\label{eq:combineLFs}
\end{align}
Once combined like this, everything proceeds as above.
Optimization, marginalization, Bayesian inference, profiling, information theory, and everything else, is just the same for this combined log-likelihood as it was for the individual log-likelihoods (above in \secref{sec:lf} and \secref{sec:nuisance}).
That is, combining data sets is extremely straightforward, \emph{so long as you have access to the likelihood functions} for each.

In the second case for combining measurements, there are $n$ data sets $Y_i$ each measuring a different parameter of interest $\theta_i$.
In this case, the goal might be to measure some new parameter $\beta$ that sets the values (or distribution of values) of the $\theta_i$.
An example from astrophysics might be that each data set $Y_i$ is the data on an exoplanet $i$, measuring the parameters $\theta_i$ of that planet; the parameters $\beta$ describe the distribution or population planet $i$.
This is sometimes called ``population inference,'' and it is generally hierarchical in form.
Here the approach is to build a likelihood for the global $\beta$ out of a product of all the individual likelihoods.
This looks like
\begin{align}
    \mathscr{L}(\beta) &= \ln p(\setof{Y}_{i=1}^n\given\beta,I)
    = \ln\prod_{i=1}^n \int p(Y_i\given\theta_i,I)\,p(\theta_i\given\beta,I)\,\dd\beta ~.\label{eq:hierarchical}
\end{align}
That combined, marginalized likelihood can be optimized to deliver a good estimator for $\beta$ or else it can be combined with priors for a hierarchical Bayesian inference, discussed in more detail elsewhere (CITE).
In detail, when any integral or product of integrals is implemented, it is important (for numerical stability) to keep all pdfs in logarithmic form, so really the integrals in \eqref{eq:hierarchical} ought to be performed with something like a \texttt{logsumexp}, and combined with a sum.\footnote{HOGG: More here?}

Sometimes what is wanted is a really simple statistic of the $\theta_i$, like the mean value of $\theta_i$.
For reasons related to what's discussed in \secref{sec:data}, if, from each data set $i$ one obtains a maximum-likelihoood estimate $\hat\theta_i$ for each parameter $\theta_i$, then these can in turn be averaged to get an estimate of the mean $\langle\theta\rangle$.
If the individual $\theta_i$ estimates are posterior estimates or the outputs of machine-learning regression, they cannot be so averaged.
A very simple example of that is shown in the appendix of \cite{goodorbad}.

Often when you are combining data, the data are coming from a scientist or a project that is upstream of you.
That is, in cases like \eqref{eq:bayes}, often you are combining measurements of $\theta_i$ coming from a project or study of which you are not a part.
HOGG: The impossibility of combining posteriors unless they (and their associated priors) are continuous functions.
HOGG: Also the impossibility of reweighting posterior samples, despite our own work. CITE LIGO RESULTS TOO.

It is critical to the validity of the sum of logarithms in \eqref{eq:combineLFs} and the product in \eqref{eq:hierarchical}
that the different data sets be \emph{independent}.
Technically this means that their pdfs are separable and that they can be multiplied.
Usually, in reality, it means that the data sets contain no objects or pixels or experiments or readouts that are in common.
They must be fully disjoint data.
The word ``independent'' can be confusing here.
Because the two data sets in \eqref{eq:combineLFs} both depend on $\theta$, the two data sets are, in a frequentist sense, covariant.
However, they are independent in the relevant sense here as long as they contain no data in common.

\section{Delivering data useful to others}\label{sec:catalogs}
Imagine that you are making a \emph{lot} of measurements; so many that you are going to do some kind of ``data release'' of your measurements, so that other people in your scientific domain can use those measurements in their own scientific projects.
What kind of measurements or estimators do you want to use to build that set or list or catalog of measurements?

For example, as we mentioned in \secref{sec:intro} and \secref{sec:data}, the \textsl{Gaia} Mission \cite{gaia} is delivering more than a billion stellar parallaxes and proper motions in a set of data releases (for example \cite{gaiadr3}).
Astronomers are using these parallaxes to estimate the distances to groups of stars in the Galaxy, and the luminosities of stars of different types.
Each of these kinds of estimates involves combining the parallaxes of multiple stars.
That is, the users of the \textsl{Gaia} Catalog want to use the catalog entries like data.
They want to make measurements using these catalog entries as data.
This situation is common in physics and astronomy:
A large catalog of measurements is used, later, as \emph{data} in a subsequent analysis.

For all the reasons given in \secref{sec:data} and \secref{sec:combining}, it is critical that the quantities given in the catalog can be used to reconstruct the likelihood functions, or approximations thereto.
That is, it is important that....
HOGG BLAH BLAH.

There is a Bayesian alternative to this view, which is the view of the most principled Bayesians I know.\footnote{In my own sphere, I attribute this view to Dan Foreman-Mackey (DeepMind); it is embodied in many of his papers and projects.}
This Bayesian view is that the goal of every paper and data release is really to deliver posterior pdfs for everything, and \emph{not} likelihoods.
However, and importantly, \emph{in addition} to these posterior pdfs, the project or paper should deliver executable code that creates those posterior pdfs.
Then the downstream user is expected not to use the posterior pdfs directly; after all:
Why would the downstream user agree with the upstream investigator's priors?
The downstream user is expected to take the executable code and modify it to their particular needs.
That is, the likelihood functions are released not as primary outputs but instead as components of executable code.
I don't necessarily disagree with this vision!
However, in practice, in my fields of study, only a tiny fraction of downstream users have completely re-run upstream code in their data analyses; a larger fraction have done things that are technically wrong (using summaries of posterior pdfs as data, say).
So while this Bayesian point of view (release posterior pdfs and also fully executable code) is self-consistent and rigorously correct, it isn't compatible with the zeitgeist in many academic disciplines.

\section{What if I don't have a likelihood?}\label{sec:lfi}
Does the requirement that there be a peak in a sensible likelihood function imply that we can \emph{only} determine whether or not we have a measurement by directly looking at the likelihood function?
The answer is \emph{no}.
Looking at the likelihood function is the best way to determine whether we have a peak in the likelihood function.
However, there are many circumstances in the contemporary sciences in which we can get some kind of estimate of our posterior pdf, but we cannot compute the likelihood function.

One example is in present-day cosmology, where one of the goals is to measure cosmological parameters using data on the large-scale structure (as traced by the positions of many millions of galaxies and quasars).
There are only a few important physical cosmological parameters, but there are (literally) billions of nuisance parameters (often wrongly called ``phases'') that must be chosen before the model makes a specific prediction for the positions of the galaxies.
Thus it is close to impossible, computationally, to compute the probability of the data given the parameters.
There are just far too many parameters.
At the same time, there are methods (known as likelihood-free inference, or simulation-based inference; \cite{abc, sbi}) for obtaining a posterior pdf for the important few parameters, without ever explicitly instantiating a likelihood function.
Is it possible to securely claim a measurement in this case?
Yes, but it requires care.

HOGG: HOW TO SAFELY ratio the posterior and the prior. MODI POINT about approximating both.
This discussion should refer back to things in \secref{sec:flat}, where HOGG HOPES that we said things about how to safely look at the ratio of the prior to the posterior in high dimensions?
Also, note that these empirical methods tend to over-estimate the information, because they tend to make the LF wiggly.

\section{Reporting the measurement}\label{sec:reporting}
What does it take to write a paper that claims that a set of data $Y$ constrains or measures a parameter (or set of parameters) $\theta$?
It should be obvious at this point that it is a likelihood function, with a peak, and with a narrow enough uncertainty, or large enough Fisher Information, that the parameter is usefully measured
(if it is not possible to access any kind of likelihood function, see \secref{sec:lfi}).
But even more important than the requirement of a peak in a likelihood function is the requirement that the paper be absolutely clear and unambiguous about everything that was assumed and done.

The art of writing a good paper about a measurement is, as in all kinds of writing, the art of having an audience in mind and being clear to that audience.
This means explaining every non-trivial decision and assumption that you have made---where ``non-trivial'' depends on the context, which in turn is set by the expectations of the audience---and showing that the likelihood function (or your method of measurement) flows directly from those decisions and assumptions.
I have taken the view recently that the decisions and assumptions should make up a stand-alone section in the papers I write (see, for example, \cite{frizzle}).
In addition, it is good practice to release all the code and data, such that the measurement can be reproduced, the assumptions can be explored, and the likelihood can be combined with others that follow (see \secref{sec:combining}).

A measurement will be more acceptable to the community the more consistent with community norms are the decisions and assumptions.
For example, imagine that in the data $Y$, the parameter of interest $\theta$ is strongly covariant with some nuisance parameter $\alpha_k$,
such that one cannot measure $\theta$ without having strong constraints on $\alpha_k$.
In this case it is possible for an investigator to deliver a measurement of $\theta$ if the investigator is willing to postulate a particular value or narrow range for $\alpha_k$.
That measurement of $\theta$ will only be accepted by the investigator's community if that community agrees that $\alpha_k$ does indeed live in or near that postulated range.
If that's controversial, then the investigator has not delivered what anyone would accept as a measurement.
For another example, if there is obviously non-Gaussian noise, but the measurement is only successful under an assumption of Gaussian noise, then there perhaps isn't a measurement in any real sense, even if there is a good peak in the assumed-Gaussian likelihood function.

A measurement, for the purposes of a scientific paper, is (in some sense) a number (the result of the measurement) and an uncertainty that can be reported in a paper abstract.
What should these two numbers be?
In an ideal world, the reported number should be the maximum $\hat\theta$ of the likelihood \eqref{eq:ML}, and the uncertainty should be related to level sets of the log-likelihood function.
The formal uncertainty on the parameter $\theta$ is the square root of the inverse of the negative of the second derivative tensor, evaluated at that maximum.
Somewhat more conservative uncertainties come by looking at likelihood ratios, or differences in log-likelihood, as discussed in \secref{sec:lf}.
Even more conservative uncertainties sometimes come from bootstrap \cite{bootstrap} or jackknife \cite{jackknife} analyses, but a full discussion of uncertainty estimation is out of scope here.

Bayesians might want to replace these with the mean (or median) of a posterior pdf and a root-variance of that.
That's fine, except for two things:
The first is that if the anything strange was used for prior pdfs, especially prior pdfs on nuisance parameters, then that weird context must additionally be reported in the abstract.
The second is that you have to live with the fact that the mean or median of posterior can, in fact, be a bad value.
If you think about non-trivial posterior pdfs (multi-modal, say), you can easily have a mean or median of posterior that is actually disfavored or ruled out by the data.
This leads many to consider ``maximum of posterior'' (MAP) estimates, which are good models by construction.
MAP values are usually not a good idea, and definitely not well estimated by MCMC samplers \cite{mcmc}, which are not in any sense optimizers; they deliver no guarantees about finding ``best'' parameter values.
Recall that Bayesians integrate; frequentists differentiate.
Posterior means and medians are found by integration.

But, to belabor the belabored, if the posterior pdf has a peak, but the likelihood function doesn't, there is no measurement.
Or maybe there \emph{is} a measurement, but it is based not just on the data $Y$ but also on some prior data or prior knowledge that is additionally relevant.
The abstract of the paper should be clear about this.

Some projects (like, for instance, the \textsl{Large Hadron Collider} Higgs particle measurement \cite{lhc}) have rules or conventions about a measurement being ``5-sigma'' or equivalent.
Strict thresholds like this are often a bad idea; after all, it is hard to know confidently that a measurement meets this criterion.
In particular, the number of sigma estimated by taking the second derivative of the log-likelihood and the number of sigma estimated by comparing the log-likelihood at the maximum to the log-likelihood at zero, will in general be different.
Which is to be preferred?
Usually the log-likelihood difference is more conservative.

Often, when a measurement is made, a report on ``how many sigma'' it is away from a standard value, or a previous measurement.
Provided that the data are independent of all previous data, this can be estimated with the difference in values divided by the sum-in-quadrature of the uncertainties of the two measurements.
If the comparison value has tiny uncertainty, or is a noise-free theoretical expectation, then likelihood ratio questions can be asked.
If the comparison value has significant uncertainty, there is no simple way to ask this without performing a joint analysis of both data sets.

When one makes a measurement, but it is consistent with zero, often ``upper limits'' are reported instead.
I strongly object to this: It is still usually possible to report a measurement and an uncertainty.
The fact that it is not significantly different from zero doesn't make it any less of a measurement.
But once again, upper limits can be constructed with log-likelihood differences, again as described in \secref{sec:lf}.

\section{Epilogue}\label{sec:discussion}
This pedagogical \documentname{} might have sounded extremely frequentist, because it is so focused on the likelihood, and not on priors and posteriors.
It is not!
The position presented here is a fully Bayesian position.
The likelihood principle is a Bayesian principle, and (additionally) the likelihood is sufficient within frequentism.
In this way, the position on measurement presented here is a position that doesn't make a commitment between frequentism and Bayesianism.
No statistical philosophy was harmed in this \documentname.

The one thing that might be controversial here, it seems to me, is my original definition in \secref{sec:intro}:
In my view, any measurement claim (about parameter $\theta$) is always contextualized in terms of one particular data set $Y$.
Is it always true that when someone says ``I have measured parameter $\theta$'' they are making a claim about a particular data set (or set of data sets)?
I believe this is true, but it is certainly possible to debate the point.

As I note above in \secref{sec:bayes}, many Bayesians feel that the only point of science is to update a posterior pdf.
This might be true, although it is not \emph{obviously} true.
Even if this point of view is true and correct, it would be the point of the literature \emph{taken as a whole}. 
It can't be the point of every single paper we write.
Any paper or project that is trying to supply data or intermediate results or, indeed, \emph{measurements} to the community must do so through likelihood functions, or approximations thereto.

\paragraph{Acknowledgments}
It is a pleasure to thank
  Dan Foreman-Mackey (DeepMind) and
  Hans-Walter Rix (MPIA)
for existential discussions about these matters over the years, and
  Gaby Contardo (Nova Gorica),
  Will Farr (Stony Brook),
  Chirag Modi (NYU),
  Alexander Novara (NYU),
  Hans-Walter Rix (MPIA), and
  Roberto Trotta (SISSA)
for valuable discussions of the content of this particular \documentname.
The Flatiron Institute is a division of the Simons Foundation.

\raggedright\footnotesize
\bibliographystyle{plain}
\bibliography{measurement}

\end{document}
