% Copyright 2025 David W. Hogg. All rights reserved.

% to-do items
% -----------
% - write all sections.
% - Make it clear who the audience is: People who know how to do data analysis, who know basic statistics and have used it. Then audit for that level.
% - give to friendlies for feedback -- especially Rix, Margossian, Marshall, Chandra, Brewer, Jaffe, Modi, Heinrich, DFM, Peebles, Beane, Audenaert, Lindegren, Malz, APW, DNS ?
% - make a plan for how we cite arXiv papers and make it consistent over the whole bibliography.
% - Fix the et al formatting in the bibliography. Heck. Fix all the formatting in the bibliography.
% - is it absolutely clear that this paper contains nothing new? It is purely pedagogical.
% - Should the abstract be about derivatives of the log likelihood instead of regions and complementary regions?
% - Check Fisher citation; also good for maximum-likelihood??
% - Is it clear that a peak in theta at fixed alpha, or theta marginalizing out alpha is not enough? It has to survive profiling? This should be in the abstract, right?
% - Fix "et al" formatting in the bibliography.
% - This paper is very relevant and ought to be cited: https://arxiv.org/pdf/2408.07700
% - Are we clear about what we are suggesting, positively, in each section?
% - Are we clear about what constitutes "data" and what we mean by "parameters"? Example of nucleosynthetic parameters vs element abundance parameters?
% - Example of LIGO populations analyses?! Should be in the Bayes part, maybe?
% - Is it clear: This paper is for someone who wants to think very rigorously about the measurement they are making.

% style notes
% -----------
% - Audit for second-person language; maybe make it more third-person where possible?
% - Is first person "I" or "we"?
% - We should use \emph{} every time we introduce a new term of jargon that is important to know.
% - Be careful with "set" vs "list" vs "vector" vs "matrix".
% - Use LF in abstract, maybe, but not the main text, which should read clean.
% - Footnotes before or after commas and periods?
% - "data" is a mass noun like "grass" or "hair."
% - Should all pdfs be conditioned on I? I think so; fix that and audit for it.

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper]{geometry}
\usepackage{setspace}
\usepackage{amsmath, amsfonts, mathrsfs}

% typesetting issues
\setstretch{1.08}
\addtolength{\topmargin}{-0.30in}
\addtolength{\textheight}{1.60in}
\setlength{\textwidth}{5.0in}
\setlength{\oddsidemargin}{0.5\paperwidth}\addtolength{\oddsidemargin}{-1.0in}\addtolength{\oddsidemargin}{-0.5\textwidth}
\pagestyle{myheadings}
\markboth{foo}{\sffamily Hogg / What is a measurement?}
\renewcommand{\newblock}{} % this adjusts the bibliography style.
\frenchspacing\sloppy\sloppypar\raggedbottom

% text macros
\renewcommand{\paragraph}[1]{\bigskip\par\noindent\textbf{#1}~---}
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\sectionname}{Section}
\newcommand{\secref}[1]{\sectionname~\ref{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}

% math macros
\newcommand{\e}{\mathrm{e}} % why do I have to define this?
\newcommand{\dd}{\mathrm{d}}
\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\norm}[1]{\lVert{#1}\rVert}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\title{\bfseries
What is a measurement?}
\author{David W. Hogg\footnote{%
DWH (david.hogg@nyu.edu) is in the Center for Cosmology and Particle Physics, Department of Physics, New York University.
He also has appointments at the Center for Computational Astrophysics, Flatiron Institute, and at the Max-Planck-Institut f\"ur Astronomie.}}
\date{2025 May}
\begin{document}
\maketitle\thispagestyle{empty}

\paragraph{Abstract}
A measurement of a parameter $\theta$ is a quantitative constraint flowing from some particular data $Y$; for example, the data from the ESA \textsl{Planck} Mission measured the matter density of the Universe.
In both Bayesian and frequentist practice, all the evidence or information in a data sample about the parameters of a model is contained in the likelihood function (LF); the LF is sufficient.
Thus if parameter $\theta$ is measured with data $Y$, there must be a neighborhood in $\theta$ in which the LF has a (sufficiently) tall peak.
Posterior pdfs, machine-learning regressions, and marginalized likelihoods (marginalized over nuisances), can all deliver things that look like measurements; these are not measurements, unless (in addition) the LF shows a peak.
Construction of the LF involves making subjective decisions and assumptions; in order for a measurement to be accepted by the community, these choices must be made in accordance with community norms.
I discuss how to demonstrate that the data $Y$ do (or can) provide a measurement of parameter $\theta$ even in the presence of substantial and covariant nuisances; it involves either profiling (optimizing over nuisances) or else taking the inverse of the Fisher information tensor.
I discuss investigator options when there is no LF, or when the LF is not tractable or not computable.

\section{Introduction}\label{sec:intro}
In the arguments about Bayesian and frequentist statistics, there are claims about subjectivity.
For example, frequentists (rightly) note that the introduction of a prior into an inference generally involves informative, subjective inputs (CITE).
For another, Bayesians (rightly) note that the calculus of probability is provably optimal for making certain kinds of probabilistic predictions or bets \cite{jaynes}; anything else involves subjective choices, at the expense of optimality.
It is increasingly recognized (despite monikers such as ``objective Bayes'' \cite{objective}) that no method for statistical inference is \emph{objectively correct}:
Every method for making a measurement, deciding among hypotheses, claiming a discovery from data, or predicting the outcome of a new experiment, involves myriad investigator choices.

This paper is partly about claims of measurement:
When is it true that a data set $Y$ has delivered a measurement of parameter $\theta$?
My answer relates to the \emph{likelihood principle}, which I discuss in \secref{sec:lf}.
This principle---agreed upon by both Bayesians and frequentists---is that all information in any data set $Y$ about any parameter $\theta$ is summarized completely by the likelihood function $p(Y\given\theta, I)$.
The likelihood function is a probability density for the data $Y$ given the parameter $\theta$ and possibly other relevant information or assumptions $I$.
The main point of this paper is that a claim of a measurement must be made---or be possible to make---using the likelihood function alone.
And, additionally, that the likelihood function must be based on assumptions or choices that represent consensus choices for the relevant scientific community.
If the claimed measurement can't be seen as a peak in a reasonably constructed likelihood function, then the claim is not (completely) substantiated by the data.
For example, the ESA \textsl{Planck} Mission \cite{Planck} delivered a measurement of the matter density of the Universe \cite{planck18};
this means that there is a tall, narrow peak in the \textsl{Planck} likelihood function in the cosmological-parameter space, and that peak corresponds to a particular value (or set of values) of the matter density.\footnote{Oddly, the \textsl{Planck} results are all shown as projections of posterior pdfs, so it isn't perfectly obvious to an outsider that in the cited paper \cite{planck18} there is indeed a peak in the likelihood. There is, though.}

One project that understands the likelihood principle is the ESA \textsl{Gaia} Mission\footnote{%
The \textsl{Gaia} Mission acts very much like it understands the likelihood principle; its data releases include quantitative descriptions of likelihood functions \cite{gaialf}.
That said, there is no documentary evidence that I can find that the project made the decisions it made specifically because of the likelihood principle.
Its design decisions may have been intuitively made, as far as I can tell, even though they were made absolutely correctly.}
\cite{gaia},
which is measuring (among many other things, the parallaxes to more than a billion stars in the Milky Way.
The parallax is a geometric measure of the inverse of the distance to the star.
This Mission is trying to deliver valuable information to the astronomical community, members of which can update their beliefs about individual stars, about stellar clusters, and about the populations of stars using the data releases.
The quantities in the \textsl{Gaia} Data Archive \cite{gaiadata} that represent its parallax measurements are maximum-likelihood parallax measurements, and uncertainty estimates based on a Fisher-matrix-like analysis of uncertainty.
These quantities can be cleanly combined into a good approximation of likelihood function \cite{gaialf}.
That is, the \textsl{Gaia} Catalog parallax and parallax uncertainty for any star are parameters of a form for the probability of the Gaia data given that star's parallax.
This is clean!
It permits any user of the Catalog to update their beliefs about that star's distance, or their beliefs about any other astrophysical inferences that involve that star's distance.
If the \textsl{Gaia} Catalog contained mean or median-of-posterior estimates or any other kind of biased estimators, the data would have been far harder (or even impossible) to use.

One result that motivated this \documentname{} is a ``cosmology with one galaxy'' claim \cite{onegalaxy}.
This claim is that there is significant information about some of the cosmological parameters---and especially the mean matter density of the Universe---in the measurable properties of a single galaxy.
This claim might be correct, but the paper does not demonstrate it.
The paper is not consistent with the likelihood principle; it does not ground its claim in the likelihood or in any proxy for the likelihood.
Instead it uses a machine-learning regression to substantiate its results; the issues with this will be discussed in \secref{sec:ml}.

I opened by noting that data analyses involve subjective choices.
There is another kind of subjectivity in play:
What method you use depends on what \emph{you want to do} with your data analysis.
The primary use case I am going to focus on is the case in which you have a data set and you have a parameter of interest, and you want to know \emph{what that particular data set has to say} about the parameter of interest.
There are other use cases.
Sometimes you are making the measurement because you want to make a \emph{prediction} for a future experiment.
That is, you want to know what some future data set will say about your parameter of interest.
Sometimes you are trying to make a \emph{decision} and your decision will depend on the value of the parameter, plus other things about your utility function (your economic goals, say).
We will discuss these alternative objectives a tiny bit in \secref{sec:bayes}, but mainly we will concentrate on answering the question ``what does my data\footnote{%
One of the questions I get asked most about data is whether the word ``data'' is singular or plural. I don't have a strong position. However, in this document I will attempt to treat the word ``data'' as a mass noun, like ``grass'' or ``hair.''}
say about my parameter of interest?''

My field is astrophysics, and many of the examples I give will be astrophysical.
Furthermore, the astrophysics community (now) is very Bayesian.
For this reason, the document that follows will seem to many of my astrophysics colleagues as oddly frequentist.
It isn't!
The likelihood principle is shared between Bayesians and frequentists:
The likelihood function is sufficient to deliver all information about the parameters coming from the data.
This is the key idea of Bayes's rule:
The prior is updated to the posterior with---and only with---the likelihood function.
Thus the question \emph{Do these data $Y$ deliver a measurement of parameter $\theta$?} must be answerable with a likelihood function---something like $p(Y\given\theta,I)$---if that likelihood function can be written down.
When the likelihood function cannot be written down, life is harder; we will address that case in \secref{sec:lfi}.

\section{The likelihood function}\label{sec:lf}
The problem setup will be that there is a list or block of data $Y$, a parameter (or list of parameters) of interest $\theta$, a list of nuisance parameters $\alpha$, and a large set of choices, assumptions, and background information $I$.
In contexts like this, there are many meanings of the word ``model,'' but for me this will be something that can be used to compute probabilities in the space of the data.
That is... HOGG
HOGG: Importantly: Nuisance parameters also!
HOGG: Importantly: Specific assumptions.

You can often (but not always) think of the likelihood function as a prediction for the expectation of the data plus a noise model.
For example, if you believe that the expectation for the data depends linearly on the parameters $(\theta, \alpha)$, and then in addition there is additive, Gaussian noise, the likelihood function becomes... HOGG
The likelihood function both tells you how you expect the data to change as you change the parameters, \emph{and} how precisely the data can be predicted, given the parameters.

I alluded to the likelihood principle in \secref{sec:intro}.
The principle is that everything about the data $Y$ that can possibly matter to the parameters $(\theta,\alpha)$ is contained in the likelihood function.
That is, if you have beliefs represented (accurately) a prior pdf $p(\theta,\alpha\given I)$ for the parameters $(\theta,\alpha)$,
and then you get data $Y$,
and then you want to update your beliefs to a posterior pdf $p(\theta,\alpha\given Y,I)$, the only legitimate procedure for updating your beliefs is to multiply by the likelihood function $p(Y\given\theta,\alpha,I)$:
\begin{align}
    p(\theta,\alpha\given Y,I) &= \frac{1}{Z}\,p(\theta,\alpha\given I)\,p(Y\given\theta,\alpha,I) ~,\label{eq:bayes}
\end{align}
where $Z$ is a normalization that ensures that $\int p(\theta,\alpha\given Y,I)\,\dd\theta\,\dd\alpha=1$.
This procedure \eqref{eq:bayes} is called ``Bayes rule'' and is the basis of Bayesian reasoning.
This is the \emph{only way} that the data are permitted to be used to update your beliefs, in Bayesian reasoning.
That's the likelihood principle.

But the likelihood principle has a role in frequentist statistics too.
The principle in frequentism is that everything about the data $Y$ that is relevant to the parameters $(\theta,\alpha)$ is encoded in the likelihood function.
That is, the likelihood function is a sufficient statistic for the data, when your goals relate (only) to the parameters $(\theta,\alpha)$.
Your best estimate $\hat{\theta}$ for the parameter $\theta$, and your uncertainty $\sigma_\theta$ on that parameter can all be derived from the likelihood function alone, for particular definitions of the word ``best.''\footnote{%
The definition of ``best'' estimator is out of scope for this document, but it relates to having minimum variance while also being asymptotically unbiased.}
The most important of these estimates is the maximum-likelihood estimate, found by maximizing the likelihood with respect to the parameter $\theta$ (and all the nuisance parameters $\alpha$).
\begin{align}
    \hat{\theta},\hat{\alpha} = \argmax_{\theta,\alpha} p(Y\given\theta,\alpha,I) ~.
\end{align}
Whatever estimate of $\theta$ you want to make, and independently of whether you (or your audience) is frequentist or Bayesian, you haven't measured your parameter $\theta$ with your data $Y$ if there isn't a peak in your likelihood function at some well-defined value of $\theta$.

In many contexts, both conceptually and for reasons of numerical precision, it is better to work with the logarithm $\mathscr{L}$ of the likelihood function;
\begin{align}
    \mathscr{L}(\theta,\alpha) &= \ln p(Y\given\theta,\alpha,I) ~.
\end{align}
When we take the log---and we always use natural logarithms here---we usually explicitly write the log likelihood in terms of the parameters only.
The data are fixed, after all.

There are two very important properties of the likelihood function, when it comes to measurement.
The first is that differences in the natural logarithm of the likelihood have an absolute meaning:
A difference of 0.5 in natural log (a likelihood ratio of $\sqrt{\e}$) between two models is something akin to ``one sigma'' and a difference of 12.5 is something akin to ``five sigma'' (HOGG CHECK).
This (it turns out) means that the second derivative of the log likelihood function with respect to parameters is related to measurement uncertainty.
This is formalized in the Cram\'er--Rao bound \cite{cramer, rao} on the variance (and bias) of estimators; more on this in \secref{sec:inference}
It also means that any claim of a measurement requires a large difference in log likelihood between a model with a best-fit value of some parameter $\theta$ and a null model in which that parameter vanishes.

That is---and critical to everything that follows in this \documentname---any claim of a measurement of any parameter $\theta$ in data $Y$ requires not just a peak in the likelihood at some value of the parameter $\theta$, but a tall peak.
The peak in the log likelihood must be taller than its surroundings by log-likelihood differences much larger than unity.\footnote{Here, and everywhere in this document, the words ``log'' or ``logarithm'' or ``logarithmic'' always refer to natural logarithms in base $\e$.}
In particular, the log likelihood difference between the measured value of $\theta$ and the values that the investigator or project claims to ``rule out'' with their data must be substantially larger than unity.

The second important property of the likelihood function is that it is parameterization-invariant.
If the problem is reparameterized from parameters $(\theta,\alpha)$ to new parameters $\beta$ such that the transformations between $(\theta,\alpha)$ and $\beta$ are invertible (and the inverse is also invertible), then the value of the likelihood at $(\theta,\alpha)$ will be precisely equal to the likelihood at the corresponding value of $\beta$.
This invariance is extremely important to frequentist statistics, and does not hold for Bayesian posteriors.
The fact that Bayesian priors and posteriors do not have reparameterization invariance will come back up in \secref{sec:bayes}.

\section{Nuisance parameters}\label{sec:nuisance}

Much of what follows is (effectively) about the nuisance parameters $\alpha$.
Nuisance parameters substantially complicate questions of measurement.
Many of the mistakes in the literature are mistakes in the handling of nuisance parameters.
This is in part because it is hard to get rid of the nuisance parameters without accidentally introducing information about the parameter of interest $\theta$.
Any information brought in that doesn't come directly from the data $Y$ is information that can deceive an investigator into mistakenly claiming a measurement or over-estimating the information in the data about the parameter of interest $\theta$.

How do we account for the nuisances $\alpha$ when we are claiming a measurement of the parameter of interest $\theta$?
For the committed Bayesians, there is only one answer, and that is to marginalize.
That is, you put a prior $p(\alpha\given I)$ on the nuisance parameters and integrate
\begin{align}
    p(\theta\given Y,I) &= \int p(\theta,\alpha\given Y,I)\,p(\alpha\given I)\,\dd\alpha ~,\label{eq:marginallf}
\end{align}
where the integral over $\alpha$ is, implicitly the integral over all of the nuisance parameter space, or the sum over all possible settings of $\alpha$ if $\alpha$ is discrete.
You might be tempted to conclude that we have a measurement of $\theta$ if there is a peak in this marginal likelihood.
That will turn out to be wrong, as we will show in \secref{sec:flat}, and related to ideas that come up in \secref{sec:ml}:
If the data contain lots of information about the nuisances $\alpha$ and the highest-likelihood values of the nuisances depend on the parameter of interest $\theta$, then these kinds of marginalizations will in general produce peaks in the marginal likelihood even when there is no information about $\theta$ in the data $Y$.
That's true even when those priors on the nuisances are flat, as we mention in \secref{sec:bayes} (HOGG DO WE?) and explicitly show in \secref{sec:flat}.

On the subject of nuisance parameters,
there is an extremely important idea in frequentist statistics, which is that of the \emph{profile likelihood}.
\begin{align}
    \mathscr{L}_\alpha(\theta) &= \max_\alpha \mathscr{L}(\theta,\alpha) ~,
\end{align}
where the subscript reminds us what we've profiled over, and the max operation means the following:
For every value of $\theta$, deliver the log likelihood for the maximum-likelihood value of $\alpha$ at that value of $\theta$.
This answers the questions of the form:
Is there \emph{any} setting of $\alpha$ that makes the data probable for this value of $\theta$?
That is, is it likely to make the data at this value of $\theta$ (when I have no opinions about $\alpha$)?
Bayesians can make a marginalized likelihood if they want to get rid of $\alpha$ (provided that they have a prior pdf on $\alpha$).
Frequentists don't usually\footnote{%
HOGG exception!}
have a pdf to apply on any parameters, so they can't marginalize; they have to optimize rather than marginalize over alpha.
Frequentists made a profile likelihood if they want to get rid of $\alpha$.\footnote{%
I like to joke that Bayesians integrate, and frequentists take derivatives. It isn't entirely a joke.}

Sometimes Bayesians do a really big integral, which is the likelihood marginalized over \emph{all} parameters,
and call it the \emph{evidence} or the \emph{fully marginalized likelihood}.
This is not relevant to our goals here, so I will leave my criticisms for a different rant.

HOGG: Something about true subjective Bayes if the community really does have shared beliefs about the nuisances. Then does the marginalized likelihood deliver a measurement? I think maybe it does?

\section{Statistical inference}\label{sec:inference}
Fundamentally, \emph{inference} is the estimation or measurement or prediction of $\theta$ given data $Y$.
For a frequentist, this involves an \emph{estimator}, which could be maximum-likelihood or otherwise.
For a Bayesian, this means making a likelihood function, and (perhaps) multiplying it by a prior and renormalizing to a posterior pdf (or sampling with MCMC \cite{mcmc}).
In what follows, for specificity, we will assume that the frequentists use maximum-likelihood estimators.
Bayesians don't necessarily have estimators, but sometimes a Bayesian will report a posterior mean or median or (wrongly\footnote{%
Recall my comment in HOGG FOOTNOTE REFERENCE that Bayesians integrate and frequentists differentiate. Bayesian beliefs are much better summarized with means or medians of posteriors than optima of posteriors: The optimum can be far from the bulk, oddly.})
a maximum of posterior.

Sometimes Bayesians like to think that, since they have a framework of prior pdf and posterior pdf, they don't need to ever choose an estimator.
There is an element of truth to this!
However even the Bayesians have to decide what number (or numbers or intervals) they \emph{report in the abstract of their paper} or in their summary slides about their parameter of interest $\theta$.
Thus even Bayesians have estimators at some level.
The qualities of estimators---in terms of bias and variance---are limited by a beautiful set of mathematics in the area of \emph{information theory}.
The most important, relevant result is the Cram\'er--Rao bound \cite{cramer, rao}, which limits the variance estimators.
This bound says that, given data $Y$, the best an unbiased estimator can do is set by the Fisher Information \cite{fisher},
One consequence of all this is that any claim about the ability of some data set $Y$ to measure or constrain some parameter or parameters of interest $\theta$ in the presence of nuisances $\alpha$ can all be boiled down to second derivatives of the log-likelihood $\mathscr{L}(\theta,\alpha)$.

Getting more specific:
It is possible to think about whether data $Y$ have provided a measurement of parameter $\theta$ by thinking about the \emph{uncertainty}\footnote{%
It's not an error bar or an error, it's an uncertainty. If it were an error, we would correct it.}
in the measurement of $\theta$.
Cram\'er--Rao guides us here:
This bound says that the best you can possibly do, in an unbiased-estimator sense, on your parameter of interest, given your data $Y$, is the square root of the inverse of the Fisher information.
That is, you start by computing the information tensor $C^{-1}_{(\theta,\alpha)}$
\begin{align}
    C^{-1}_{(\theta,\alpha)} &= - \frac{\dd^2\mathscr{L}}{\dd(\theta,\alpha)^2} ~.\label{eq:fisher}
\end{align}
This second-derivative tensor is $d\times d$, where $d$ is the total number of elements in the full set of parameters $(\theta,\alpha)$.
HOGG DO WE HAVE DEFINITIONS for the dimensionality of $\theta$ and $\alpha$?
Then you invert this matrix to get $C_{(\theta,\alpha)}$.
Then you take the (square) submatrix of $C_{(\theta,\alpha)}$ corresponding to $\theta$ only.\footnote{It is critical here that you invert before you take the submatrix. If you submatrix before you invert, you will get disastrously wrong uncertainty estimates. In that (wrong) case you have computed the best possible uncertainty under the (wrong) assumption that God told you the correct values of the nuisance parameters.}
If there is only one, scalar parameter of interest $\theta$, then this gives the uncertainty variance (the square of the uncertainty) on $\theta$.
If $\theta$ has more than one component, this submatrix is the variance tensor of the uncertainty ellipse in the $\theta$ space.
HOGG DO WE NEED SOME PICTURES OR DIAGRAMS?
Importantly, this bound sets the properties of any possible estimator.
If this Cram\'er--Rao (or Fisher-matrix) uncertainty is larger than what you need to claim a (say 5-sigma) measurement, then \emph{there is simply no sense in which data $Y$ have measured parameter $\theta$}.

Second derivatives like in \eqref{eq:fisher} sound hard!
But they aren't, for two reasons.
The first is that, when your model is linear, and your noise is Gaussian, this second derivative becomes an outer product of first derivatives (see, for example, \cite{fittingaline}).
The second is that we live in the future, in which programming languages like jax \cite{jax} give you analytic derivatives to second order for free.

Frequentists and Bayesians are all bound by Cram\'er--Rao.
However, Bayesians can do more than frequentists.
Bayesians can update beliefs, compute optimal betting odds on future outcomes, and optimize utility over decisions, integrating over all possible states of the world.
All these things are important, but they don't change the fundamental point that the likelihood function is sufficient for conveying the information in data $Y$ about parameter $\theta$ and thus the likelihood function is the only thing in play when a claim of measurement is being made.

\section{Isn't Bayes The Best Thing To Do (tm)?}\label{sec:bayes}
The most beautiful thing about Bayesian inference is that it is the provably correct way for an individual to reason about uncertain things.\footnote{%
The theorems are known as the Cox theorems.
The best explanation of them I know is the first Chapter of Jaynes's book \cite{jaynes}.}
The rules of probability theory are also the rules about reasoning about the plausibility of the things you know, or want to know.
For this reason, it makes sense for a scientist---or a human being---to reason in a Bayesian manner.
I would even say (and I have said elsewhere \cite{plausibility})
that, for the reason of this provable correctness, the whole scientific community ought to act, collectively, in a kind of Bayesian way.
It should have a collective set of beliefs and it should update those collective beliefs by (at least approximately) multiplying by a likelihood function for the new data it obtains each year.

Does this mean that \emph{every scientific paper} should describe a prior pdf, a likelihood, and a posterior obtained by Bayes's rule?
No, it does not.
The point of each scientific paper is \emph{not}---or not mainly---to describe to the community \emph{the authors' beliefs}.
The point of each scientific paper is to move forward the beliefs of the community of scientists.
That moving forward could be contributions to, or arguments about, the community's prior beliefs, the community's many likelihood functions, and the community's posterior beliefs.
Each paper has a small role in this program; it is not the case that each paper executes the whole program for the whole community.\footnote{%
There are many treatises on the philosophy of science.
I feel like we need a philosophical theory of the \emph{scientific paper} or of the \emph{individual scientific contribution}.}

When scientist A reads a paper written by scientist B, what does scientist A hope to get from that paper, and what did scientist B hope to provide in that paper?
I believe that scientist A wants to know if scientist A's beliefs need to be updated.
I believe that scientist B wants to influence the beliefs of scientist A.
In either case, what scientist A needs to get from the paper is not scientist B's posterior pdf.
What scientist A needs to get from the paper is scientist B's likelihood function (or, alternatively, scientist B's data; we will return to that idea below in \secref{...} HOGG DO WE?).

If scientist A can understand clearly the assumptions going into scientist B's likelihood function,
and if scientist A broadly agrees with them,
then scientist A can simply update their beliefs by multiplying (approximately or as exactly as possible, depending on the nature of the work)
scientist A's prior pdf with scientist B's likelihood function.
And it is worthy of note that scientist A is much more prone to agree with the assumptions going into scientist B's likelihood function if scientist B's assumptions are aligned with community norms in the field.

When I say that scientist A will multiply their prior pdf by scientist B's likelihood function,
I don't necessarily mean that they will do so by writing a piece of code that imports or reimplements prior pdfs, imports the likelihood function, and literally does the multiply and representation of the output.
I mean something more approximate and vague.
But sometimes it \emph{literally is the case} that this code and import and multiply \emph{is} what scientist A does.
In the case from \secref{sec:intro} with the ESA \textsl{Gaia} Catalog, that is exactly what we have been doing.
And NASA created the \textsl{LAMBDA} archive in part to preserve executable likelihood functions in cosmology for exactly this purpose \cite{lambda}.

Of course this is a dream.
In many real cases, scientist A will not agree with the assumptions going into scientist B's likelihood function.
But usually scientist A will agree with a lot of those assumptions.
Then scientist B's paper is a template for making a new likelihood function with modified assumptions, which scientist A can then use to modify scientist A's beliefs.
This is a less direct use of scientist B's likelihood function, but still very useful.

The point of scientist B's paper is not to represent scientist B's beliefs, but instead to aid scientist A in \emph{updating} scientist A's beliefs, or maybe the beliefs of some community of which scientist A is a part.
Beliefs of individual scientists (or even whole scientific communities) are idiosyncratic and subjective.
If we want to update someone \emph{else's} beliefs, we must provide them with a likelihood function.
The point of (most of) the scientific literature is to communicate about (aspects of) likelihood functions.
When a paper multiplies a prior by a likelihood to produce a posterior it is---at best---demonstrating the effect of the data on typical beliefs.
At worst it is irrelevant to every individual reader.\footnote{%
There is only one important sense in which such a paper---a paper that takes one investigator's subjective prior beliefs and multiplies them by one data set's likelihood function to deliver the updated investigator's subjective posterior beliefs---is a useful scientific product.
It is that it demonstrates the procedure of constructing a prior pdf, constructing a sensible (we hope) likelihood function, and combining them into a posterior pdf.
That demonstration can be useful to others, especially if the methods are unambiguously explained and the codes are released for re-use.
But the specific value or shape of the individual investigator's particular posterior pdf is not an object of community scientific interest, unless the author is an \emph{extremely important person}.}

HOGG: HERE is where we would talk about LIGO posterior samplings and LIGO populations inferences.
And the point / result that if the number of sources gets large, current methods don't work!

This \sectionname{} opened with the point that Bayesian reasoning is optimal.
In what very specific sense is it optimal?
It turns out that it is best for \emph{placing bets};
One statement of the theorems is that if two bettors have access to the same information, and one reasons via Bayes, and one reasons in any other way, the Bayesian bettor will win bets (in the long run) against the other.
Thus Bayes is optimal for making predictions, and in particular putting betting odds on predictions.
This explains why Bayesian reasoning was developed mainly by the insurance industry \cite{bayesactuary}.
Science involves prediction, crucially.
However, measurements are not the same as predictions.
They can be similar though: A measurement can (in principle) be seen as a prediction for what a subsequent experiment might measure.

There is another use case for Bayesian reasoning, which is closely related, and it is decision-making:
If you have to make a decision (which side of a bet to take, which model to prefer, what to have for lunch, and so on),
and if you can specify your \emph{utility function}, you can use Bayesian posteriors to compute expectations of utility.
This is out of scope for this document, but briefly your utility is the amount of money (or money equivalent) that you make or lose in different future outcomes, as a function of your decision.
Again it makes sense that the insurance industry would care.
But again, measurements are not the same as decisions.\footnote{%
It might seem obvious that measurements are not decisions!
But in fact any paper about any measurement says things in its abstract (and body) about the value obtained by the measurement process, and the assumptions that went into the measurement, and what should or could be concluded from that measurement.
Those are choices, which can be phrased as decisions.
So even if we are reporting a measurement based only on the likelihood function, perhaps we should be Bayesian when we think about what words to write where and in what order?}

The theorems about Bayesian reasoning are correct of course (they're theorems!).
You, personally, ought to reason according to Bayes (for example when you are deciding what to order at a restaurant, negotiating salary, or deciding whether to buy insurance).
Similarly, the scientific community taken as a whole ought to update its beliefs (inasmuch as a community has beliefs) according to Bayes.
The important thing is that \emph{a measurement is not the same as a belief}.
Measurements are used to \emph{update} your beliefs.
Hence measurements are properties of your likelihood function, not properties of your posterior pdf.
That's almost the main point of this \documentname.

Finally, I remark that people often say to me: ``But Bayesian inference with flat priors is identical to maximum likelihood, so it doesn't matter.''
This is absolutely \emph{not true}.
For one, Bayesian inference produces probability distributions over outcomes, not single outcomes.
For another, Bayesian inferences depend on the parameterization; while maximum likelihood estimates don't depend on reparameterizations, the most posterior-probable parameter combinations will always depend on parameterization.
For yet another, even flat priors can introduce very weird artifacts and biases into your measurements.
I give one very simple and natural example below in \secref{sec:flat}.
If you don't want priors to bias your measurement, \emph{don't use priors}.

\section{Can't I just use flat priors?}\label{sec:flat}
We need a peak in the likelihood function to claim a measurement.
But if we want to be Bayesian, and we want to use \emph{flat priors},\footnote{%
If you want to be Bayesian, there is almost no context in which you would want to use flat priors on any parameter you care about.
The Cox theorems that deliver Bayesian reasoning only apply when the Bayesian procedure is being used to update \emph{your beliefs}.
It is essentially impossible that your belief about anything you care about is well represented by a flat pdf.}
isn't it the case that any peak in the likelihood will lead to an identically shaped peak in the posterior pdf?

This sounds so naively and obviously true, what could possibly go wrong?
There are two things that are very wrong about this idea---the idea that if you use flat priors its the same as having just a likelihood function.
The first thing that is wrong is that (as we emphasized in \secref{sec:lf}) the likelihood function is reparameterization-invariant.
The posterior pdf is absolutely not reparameterization-invariant.
So if the prior is flat in some parameter, it is absolutely not flat in any non-trivial function of that parameter.
Even worse, if it is flat in some parameter, in any square-root or square or logarithm of that parameter, the prior is strongly peaked \emph{at one edge of the allowed range!}
That's demonstrated in FIGURE HOGG and definitely not what people usually have in mind when they think about using ``uninformative priors.''

The second thing that is wrong about this idea is that when the number of parameters gets large---larger than 2 say---it becomes very easy to believe that there is a peak in the likelihood function when there most definitely isn't.
To demonstrate this, we generate a very simple toy-likelihood and toy-posterior situation, in which all priors are flat, and in which it very very much looks like there is a peak in the likelihood function when there absolutely is not.

In our toy problem we have data $Y$ that makes up a $n\times 1$ column vector, or an ordered list of $n$ elements.
We have parameters $\theta$, which make up a $p\times 1$ column vector, or an ordered list of $p$ elements.
If we use the symbol $\theta_i$ to denote the $i$th element ($1\leq i\leq p$) of this vector, we could say that $\theta_1$ is the parameter of interest that we care about and the other $\theta_i$ (for $i>1$) are the nuisance parameters $\alpha$ (HOGG AUDIT NOTATION) that we don't care about.
The simplest possible toy model is that the LF depends only on certain linear combinations of the parameters $\theta$.
That is, there is a rectangular $q\times p$ matrix $A$, with $q<p<n$, such that, to some very good approximation,
\begin{align}
    \mathscr{L}(\theta) \equiv \ln p(Y\given\theta,I) &= K - \frac{1}{2}\,\norm{A\cdot\theta - a}^2 ~,\label{eq:toyLF}
\end{align}
where $K$ is some scalar constant, $a$ is a $q$-vector constant, and the norm is the standard euclidean norm (square root of sum of squares).
This model might be called ``low rank'' or ``degenerate'':
There are parameter degeneracies such that the likelihood depends on less than all the parameters.
And, by the way, this toy example is extreme; we don't need these parameter degeneracies to be exact to get the effect we are about to see.

HOGG FIX THIS to be more like a standard linear LF, but with a design matrix that is LOW RANK; rank $q$ with $q<p<n$.

(HOGG DESCRIBE PRIOR by adjusting the following:)
When we do Bayesian inference, we will want a prior, and, because we want to be maximally uninformative, we will choose a prior that is precisely flat in all parameters $\theta_i$.
That is
\begin{align}
    p(\theta\given I) &= \prod_{i=1}^p U(\theta_i\given a_i, b_i) \\
    U(x\given a, b) & = \left\{\begin{array}{cl}
    0 & \mbox{for $x<a$,} \\
    (b-a)^{-1} & \mbox{for $a<x<b$,} \\
    0 & \mbox{for $b<x$,}\end{array}\right.
\end{align}
where the $a_i, b_i$ ($1\leq i\leq p$) are prior limit parameters with $a_i<b_i$ for all $i$.
In fact, these flat priors \emph{aren't} uninformative (CITE THINGS), but perhaps, in some circumstances, they are least informative?
It turns out that they will be quite informative in what follows.
This prior requires us to make $2\,p$ choices, to wit, the upper and lower limits $a_i, b_i$ for each parameter $\theta_i$.

HOGG SHOW RESULTS.

There is nothing really new about this demonstration, by the way.
They are all simple consequences of prior-volume effects or prior-edge effects discussed in many places (CITE THINGS).
This demonstration also links to the idea that flat priors aren't in fact uninformative, as has been argued elsewhere in many places (CITE THINGS).
HOGG: ALSO this situation is very easy to diagnose (prior-dependence of results, moving the nuisance priors).

\section{Machine learning or regression}\label{sec:ml}
There are many ways in which machine learning might enter into a project to measure a parameter $\theta$ given data $Y$.
Here we will only consider one, which is \emph{supervised regression}:
The setup for supervised regression is that there is a \emph{training set} $\setof{Y_i, \theta_i}_{i=1}^n$ of $n$ data instances $Y_i$ and corresponding labels $\theta_i$.
We are going to \emph{learn} a flexible function $f(Y;W)$, parameterized by a large block of weights $W$, that does a good job of predicting labels $\theta_i$ for training-set examples $Y_i$.
We will estimate our parameter of interest $\theta$ by executing the learned function on our data $Y$.
In equations, this looks something like
\begin{align}
    \hat{W} &= \argmin_W \sum_i\norm{\theta_i - f(Y_i;W)} \\
    \hat{\theta} &= f(Y;\hat{W}) ~,
\end{align}
where $\norm{x}$ is some kind of metric distance (like the Euclidean norm or the square of that).

Ideally the labels $\theta_i$ in your training set are very reliable and accurate estimates for each training data object $Y_i$.
If they are noisy, pure regressions aren't a good idea; it makes more sense to go to models with a generative aspect (for example, CITE).
Ideally the data $Y_i$ in your training set are somehow ``drawn'' from the same distribution as the data $Y$ you have.
If they are not drawn from the same distribution, strong biases will occur.
Often it is believed that so long as the training data ``cover'' the test data (for example the test data live inside the convex hull of the training data), then things are fine.
This isn't true, and besides, once the data get large enough, coverage of that kind is impossible to satisfy.

It turns out that, even when the situation is ideal---%
even when the training labels are perfect, the data are drawn from the same distribution as the training data, and the function $f()$ has been given sufficient flexibility---the results of machine learning regressions are biased.
Full analysis of this is out of scope; it is discussed extensively elsewhere (for example, CITE).
It relates to the point made above in \secref{sec:bayes}:
Good predictions (like those produced by Bayesian inferences) are biased, and machine learning regressions are optimized to make good predictions.
The connections between machine learning regressions and Bayesian inferences are strong:
Machine learning regressions produce results with a lot of properties similar to maximum \foreign{a posteriori} (maximum-of-posterior) parameter estimates, with the training data serving as an implicit prior.

Is a machine learning regression output a measurement?
No, it is not, or at least not in itself:
When a machine learning method returns a result, it is using information from your data $Y$
but also from all of the training data $\setof{Y_i, \theta_i}_{i=1}^n$.
That is, it cannot be described as telling you what, in your data $Y$, you have learned about parameter $\theta$.

Another way to see this is:
A regression, fundamentally, is a kind of nearest-neighbor method.
Different regression methods are only different in their sophistication about finding neighbors.
The fundamental assumption underlying a regression is that if two data sets $Y$ and $Y_i$ are ``similar'' in some important way, then their labels $\theta$ and $\theta_i$ must also be similar in some sense.
This might be true!
But it can be true for two reasons.
It can be true because the data $Y$ really contains information about the label $\theta$.
Or it can be true because objects that are similar in one way are similar in many other ways.
That is, the fact that a regression to determine label $\theta$ given data input $Y$ works well does not, in itself, imply that there is information about $\theta$ in $Y$.

To be concrete: Imagine that data $Y$ are very informative about some quantity or label $\gamma$.
And imagine that, in the training set, objects with high $\theta$ also tend to have high $\gamma$, and \foreign{vice versa}.
Then a regression trained on the training set will deliver good predictions of labels $\theta$, \emph{whether or not} the data $Y$ contains information about $\theta$ directly.
The regression does not return measurements of $\theta$.
It returns predictions, under the assumption that the training set is representative.

\section{What is my data?}\label{sec:data}
HOGG: As raw as possible.

HOGG: If not raw, then the derived data should be related to the raw data only through a LF. Why? Proofs.

HOGG: Explicit re linear chi-squared fitting: It is associative in the relevant sense!

HOGG: Using posteriors does not work; call out the exoplanet atmosphere crew.

HOGG: If you are working with derived data, be careful about repeating the data or using it twice.

\section{Combining data samples}\label{sec:combining}
There are (at least) two cases for combining measurements.
In the first, there are two data sets, $Y_1$ and $Y_2$, and they both provide measurements of the parameters $\theta$ of interest, plus maybe nuisance parameters $\alpha_1$ and $\alpha_2$ (which in general will differ between $Y_1$ and $Y_2$).
An example from cosmology is the cosmological parameters $\theta$, which can be constrained with the power spectrum of the cosmic microwave background (which could be data $Y_1$) and correlation function of galaxies (which could be data $Y_2$); each of these two kinds of data have two different kinds of nuisance parameters, but they both contain information about $\theta$.

In this case, the goal might be to get the best possible measurement of the parameters $\theta$ given both data sets.
The great thing is---as long as the two data sets were created or made independently---the likelihood functions just multiply (the log likelihoods just add), such that
\begin{align}
    \mathscr{L}(\theta,\alpha_1,\alpha_2) &= \ln p(Y_1\given\theta,\alpha_1,I) + \ln p(Y_2\given\theta,\alpha_2,I) ~.\label{eq:combineLFs}
\end{align}
Once combined like this, everything proceeds as above.
Optimization, marginalization, Bayesian inference, profiling, information theory, and everything else, is just the same for this combined log-likelihood as it was for the individual log-likelihoods (above in \secref{sec:lf} and \secref{sec:nuisance}).
That is, combining data sets is extremely straightforward, \emph{so long as you have access to the likelihood functions} for each.

In the second case for combining measurements, there are $n$ data sets $Y_i$ each measuring a different parameter of interest $\theta_i$.
In this case, the goal might be to measure some new parameter $\beta$ that sets the values (or distribution of values) of the $\theta_i$.
An example from astrophysics might be that each data set $Y_i$ is the data on an exoplanet $i$, measuring the parameters $\theta_i$ of that planet; the parameters $\beta$ describe the distribution or population planet $i$.
This is sometimes called ``population inference,'' and it is generally hierarchical in form.
Here the approach is to build a likelihood for the global $\beta$ out of a product of all the individual likelihoods.
This looks like
\begin{align}
    \mathscr{\beta} &= \ln p(\setof{Y}_{i=1}^n\given\beta,I) \\
    p(\setof{Y}_{i=1}^n\given\beta,I) &= \prod_{i=1}^n \int p(Y_i\given\theta_i,I)\,p(\theta_i\given\beta,I)\,\dd\beta ~.\label{eq:hierarchical}
\end{align}
That combined, marginalized likelihood can be optimized to deliver a good estimator for $\beta$ or else it can be combined with priors for a hierarchical Bayesian inference, discussed in more detail elsewhere (CITE).
In detail, when any integral or product of integrals is implemented, it is important (for numerical stability) to keep all pdfs in logarithmic form, so really the integral \eqref{eq:hierarchical} ought to be performed with something like a \texttt{logsumexp}.\footnote{HOGG: More here?}

Sometimes what is wanted is a really simple statistic of the $\theta_i$, like the mean value of $\theta_i$.
For reasons related to what's discussed in \secref{sec:data}, if, from each data set $i$ one obtains a maximum-likelihoood estimate $\hat\theta_i$ for each parameter $\theta_i$, then these can in turn be averaged to get an estimate of the mean $\langle\theta\rangle$.
If the individual $\theta_i$ estimates are posterior estimates or the outputs of machine-learning regression, they cannot be so averaged.
A very simple example of that is shown in the appendix of \cite{goodorbad}.

Often when you are combining data, the data are coming from a scientist or a project that is upstream of you.
That is, in cases like \eqref{eq:bayes}, often you are combining measurements of $\theta_i$ coming from a project or study of which you are not a part.
HOGG: The impossibility of combining posteriors unless they (and their associated priors) are continuous functions.
HOGG: Also the impossibility of reweighting posterior samples, despite our own work. CITE LIGO RESULTS TOO.

It is critical to the validity of the sum of logarithms in \eqref{eq:combineLFs} and the product in \eqref{eq:hierarchical}
that the different data sets be \emph{independent}.
Technically this means that their pdfs are separable and that they can be multiplied.
Usually, in reality, it means that the data sets contain no objects or pixels or experiments or readouts that are in common.
They must be fully disjoint data.
The word ``independent'' can be confusing here.
Because the two data sets in \eqref{eq:combineLFs} both depend on $\theta$, the two data sets are, in a frequentist sense, covariant.
However, they are independent in the relevant sense here as long as they contain no data in common.

\section{Delivering data useful to others}\label{sec:catalogs}
Imagine that you are making a \emph{lot} of measurements; so many that you are going to do some kind of ``data release'' of your measurements, so that other people in your scientific domain can use those measurements in their own scientific projects.
What kind of measurements or estimators do you want to use to build that set or list or catalog of measurements?

HOGG: FOR EXAMPLE..

For all the reasons given in \secref{sec:data} and \secref{sec:combining}, it is critical that the quantities given in your catalog can be used to reconstruct the likelihood functions, or approximations thereto.
HOGG BLAH BLAH. Also refer back to \secref{sec:intro} where the Gaia Catalog was discussed.

HOGG: Show that least-square fitting is transitive in the right way (and everything else is not). Email with Novara 2025-05-09.

\section{What if I don't have a likelihood?}\label{sec:lfi}
Does the requirement that there be a peak in a sensible likelihood function imply that we can only determine whether or not we have a measurement by directly looking at the likelihood function?
Obviously, looking at the likelihood function is the best way to determine whether we have a peak in the likelihood function.
However, there are many circumstances in the contemporary sciences in which we can get some kind of estimate of our posterior pdf, but we cannot compute the likelihood function.

One example is in present-day cosmology, where one of the goals is to measure cosmological parameters using data on the large-scale structure (as traced by the positions of many millions of galaxies and quasars).
There are only a few important physical cosmological parameters, but there are (literally) billions of nuisance parameters (often wrongly called ``phases'') that must be chosen before the model makes a specific prediction for the positions of the galaxies.
Thus it is close to impossible, computationally, to compute the probability of the data given the parameters.
There are just far too many parameters.
At the same time, there are methods (known as likelihood-free inference, or simulation-based inference; \cite{abc, sbi}) for obtaining a posterior pdf for the important few parameters, without ever explicitly instantiating a likelihood function.
Is it impossible to securely claim a measurement in this case?
It is not.

HOGG: HOW TO SAFELY ratio the posterior and the prior. MODI POINT about approximating both.
This discussion should refer back to things in \secref{sec:flat}, where HOGG HOPES that we said things about how to safely look at the ratio of the prior to the posterior in high dimensions?

\section{Writing up the measurement in a paper}\label{sec:claim}
What does it take to write a paper that claims that a set of data $Y$ constrains or measures a parameter (or set of parameters) $\theta$?
It should be obvious at this point that it is a likelihood function, with a peak, and with a narrow enough uncertainty, or large enough Fisher Information, that the parameter is usefully measured
(if it is not possible to access any kind of likelihood function, see \secref{sec:lfi}).
But even more important than the requirement of a peak in a likelihood function is the requirement that the paper be absolutely clear and unambiguous about everything that was assumed and done.

The art of writing a good paper about a measurement is, as always, the art of writing clearly and with the audience in mind.
This means explaining every non-trivial decision and assumption that you have made---where ``non-trivial'' depends on the context, which in turn is set by the expectations of the audience---and showing that the likelihood function (or your method of measurement) flows directly from those decisions and assumptions.
I have taken the view recently that the decisions and assumptions should make up a stand-alone section in the papers I write (see, for example, \cite{frizzle}).
In addition, it is good practice to release all the code and data, such that the measurement can be reproduced, the assumptions can be explored, and the likelihood can be combined with others that follow (see \secref{sec:combining}).

A measurement will be more acceptable to the community the more consistent with community norms are the decisions and assumptions.
For example, imagine that in the data $Y$, the parameter of interest $\theta$ is strongly covariant with some nuisance parameter $\alpha_k$,
such that one cannot measure $\theta$ without having strong constraints on $\alpha_k$.
In this case it is possible for an investigator to deliver a measurement of $\theta$ if the investigator is willing to postulate a particular value or narrow range for $\alpha_k$.
That measurement of $\theta$ will only be accepted by the investigator's community if the community agrees that $\alpha_k$ does indeed live in or near that postulated range.
If that's controversial, then the investigator has not delivered what anyone would accept as a measurement.
For another example, if there is obviously non-Gaussian noise, but the measurement is only successful under an assumption of Gaussian noise, then there perhaps isn't a measurement in any real sense, even if there is a good peak in the assumed-Gaussian likelihood function.

HOGG: What do I write in the abstract of my paper? What uncertainty do I report? Answer this for all statistical philosophies.

HOGG: How many ``sigma'' is my measurement? This is a requirement in many fields of physics.

HOGG: Note that if you have a posterior peak but you can't see a peak in the likelihood function, it doesn't mean that you don't have a measurement! It just means that the scope of your likelihood function is too narrow. Maybe more data were involved. Be explicit about that and you will be okay.

\section{Discussion}\label{sec:discussion}
This might have sounded frequentist, but it is not!
This is a fully Bayesian position.
Actually, it is a position that doesn't make a commitment between frequentism and Bayesianism.

Many Bayesians feel that the only point of science is to update a posterior. Maybe?! But that would be the point of the literature taken as a whole. Not the point of every single paper you write.
If you want to support your Bayesian readers, give them a likelihood function!

If you put tons of work into making a likelihood function, why obscure it by multiplying it by a pointless and obscuring prior?

\paragraph{Acknowledgments}
It is a pleasure to thank
  Dan Foreman-Mackey (DeepMind) and
  Hans-Walter Rix (MPIA)
for existential discussions about these matters over the years, and
  Chirag Modi (NYU),
  Alexander Novara (NYU), and
  Hans-Walter Rix (MPIA)
for valuable discussions of the content of this particular \documentname.
The Flatiron Institute is a division of the Simons Foundation.

\raggedright
\bibliographystyle{plain}
\bibliography{measurement}

\end{document}
