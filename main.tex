% Copyright 2025 David W. Hogg. All rights reserved.

% to-do items
% -----------
% - write all sections.
% - Make it clear who the audience is: People who know how to do data analysis, who know basic statistics and have used it. Then audit for that level.
% - give to friendlies for feedback -- especially Rix, Margossian, Marshall, Chandra, Brewer, Jaffe, Modi, Heinrich?
% - make a plan for how we cite arXiv papers and make it consistent over the whole bibliography.
% - Fix the et al formatting in the bibliography. Heck. Fix all the formatting in the bibliography.
% - is it absolutely clear that this paper contains nothing new? It is purely pedagogical.
% - Should the abstract be about derivatives of the log likelihood instead of regions and complementary regions?
% - Check Fisher citation; also good for maximum-likelihood??
% - Is it clear that a peak in theta at fixed alpha, or theta marginalizing out alpha is not enough? It has to survive profiling? This should be in the abstract, right?
% - Fix "et al" formatting in the bibliography.
% - This paper is very relevant and ought to be cited: https://arxiv.org/pdf/2408.07700
% - Are we clear about what we are suggesting, positively, in each section?
% - Are we clear about what constitutes "data" and what we mean by "parameters"? Example of nucleosynthetic parameters vs element abundance parameters?
% - Example of LIGO populations analyses?! Should be in the Bayes part, maybe?

% style notes
% -----------
% - Audit for second-person language; maybe make it more third-person where possible?
% - Is first person "I" or "we"?
% - We should use \emph{} every time we introduce a new term of jargon that is important to know.
% - Be careful with "set" vs "list" vs "vector" vs "matrix".
% - Use LF in abstract, maybe, but not the main text, which should read clean.
% - Footnotes before or after commas and periods?
% - "data" is a mass noun like "grass" or "hair."
% - Should all pdfs be conditioned on I? I think so; fix that and audit for it.

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper]{geometry}
\usepackage{setspace}
\usepackage{amsmath, amsfonts, mathrsfs}

% typesetting issues
\setstretch{1.08}
\addtolength{\topmargin}{-0.30in}
\addtolength{\textheight}{1.60in}
\setlength{\textwidth}{5.00in}
\setlength{\oddsidemargin}{3.25in}\addtolength{\oddsidemargin}{-0.5\textwidth}
\pagestyle{myheadings}
\markboth{foo}{\sffamily Hogg / What is a measurement?}
\renewcommand{\newblock}{} % this adjusts the bibliography style.
\frenchspacing\sloppy\sloppypar\raggedbottom

% text macros
\renewcommand{\paragraph}[1]{\bigskip\par\noindent\textbf{#1}~---}
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\sectionname}{Section}
\newcommand{\secref}[1]{\sectionname~\ref{#1}}

% math macros
\newcommand{\e}{\mathrm{e}} % why do I have to define this?
\newcommand{\dd}{\mathrm{d}}
\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}

\title{\bfseries
What is a measurement?}
\author{David W. Hogg\footnote{%
DWH (david.hogg@nyu.edu) is in the Center for Cosmology and Particle Physics, Department of Physics, New York University.
He also has appointments at the Center for Computational Astrophysics, Flatiron Institute, and at the Max-Planck-Institut f\"ur Astronomie.}}
\date{2025 March}
\begin{document}

\maketitle\thispagestyle{empty}

\paragraph{Abstract}
A measurement of a parameter $\theta$ is a quantitative constraint on the parameter flowing from some particular data $Y$; for example, the data from the ESA \textsl{Planck} Mission measured the matter density of the Universe.
In both Bayesian and frequentist practice, all the evidence or information in a data sample about the parameters of a model is contained in the likelihood function (LF); the LF is sufficient.
Thus if parameter $\theta$ is measured with data $Y$, there must be a neighborhood in $\theta$ in which the LF has a (sufficiently) tall peak.
Posterior pdfs, machine-learning regressions, and marginalized likelihoods (marginalized over nuisances), can all deliver things that look like measurements even when the raw LF is not peaked; these do not constitute measurements.
Construction of the LF involves making subjective decisions and assumptions; in order for a measurement to be accepted by the community, these choices must be made in accordance with community norms.
I discuss how to demonstrate that the data $Y$ do (or can) provide a measurement of parameter $\theta$ even in the presence of substantial and covariant nuisances; it involves either profiling (optimizing over nuisances) or else inspection of the inverse of the Fisher information tensor.
I discuss investigator options when there is no LF, or when it is intractable to search, optimize, or differentiate the LF.

\section{Introduction}\label{sec:intro}
In the arguments about Bayesian and frequentist statistics, there are claims about subjectivity.
For example, frequentists (rightly) note that the introduction of a prior into an inference generally involves informative, subjective inputs (CITE).
For another, Bayesians (rightly) note that the calculus of probability is provably optimal for making certain kinds of probabilistic predictions or bets \cite{jaynes}; anything else involves subjective choices, at the expense of optimality.
It is increasingly recognized (despite monikers such as ``objective Bayes'' \cite{objective}) that no method for statistical inference is \emph{objectively correct}:
Every method for making a measurement, deciding among hypotheses, claiming a discovery from data, or predicting the outcome of a new experiment, involves myriad investigator choices.

This paper is partly about claims of measurement:
When is it true that a data set $Y$ has delivered a measurement of parameter $\theta$?
My answer relates to the \emph{likelihood principle}, which I discuss in \secref{sec:lf}.
This principle---agreed upon by both Bayesians and frequentists---is that all information in any data set $Y$ about any parameter $\theta$ is summarized completely by the likelihood function $p(Y\given\theta, I)$.
The likelihood function is a probability density for the data $Y$ given the parameter $\theta$ and possibly other relevant information or assumptions $I$.
The main point of this paper is that a claim of a measurement must be made---or be possible to make---using the likelihood function alone.
And, additionally, that the likelihood function must be based on assumptions or choices that represent consensus choices for the relevant scientific community.
If the claimed measurement can't be seen as a peak in a reasonably constructed likelihood function, then the claim is not (completely) substantiated by the data.
For example, the ESA \textsl{Planck} Mission \cite{Planck} delivered a measurement of the matter density of the Universe \cite{planck18};
this means that there is a tall, narrow peak in the \textsl{Planck} likelihood function in the cosmological-parameter space, and that peak corresponds to a particular value (or set of values) of the matter density.\footnote{Oddly, the \textsl{Planck} results are all shown as projections of posterior pdfs, so it isn't perfectly obvious to an outsider that in the cited paper \cite{planck18} there is indeed a peak in the likelihood. There is, though.}

One project that understands the likelihood principle is the ESA \textsl{Gaia} Mission\footnote{%
The \textsl{Gaia} Mission acts very much like it understands the likelihood principle; its data releases include quantitative descriptions of likelihood functions \cite{gaialf}.
That said, there is no documentary evidence that I can find that the project made the decisions it made specifically because of the likelihood principle.
Its design decisions may have been intuitively made, as far as I can tell, even though they were made absolutely correctly.}
\cite{gaia},
which is measuring (among many other things, the parallaxes to more than a billion stars in the Milky Way.
The parallax is a geometric measure of the inverse of the distance to the star.
This Mission is trying to deliver valuable information to the astronomical community, members of which can update their beliefs about individual stars, about stellar clusters, and about the populations of stars using the data releases.
The quantities in the \textsl{Gaia} Data Archive \cite{gaiadata} that represent its parallax measurements are maximum-likelihood parallax measurements, and uncertainty estimates based on a Fisher-matrix-like analysis of uncertainty.
These quantities can be cleanly combined into a good approximation of likelihood function \cite{gaialf}.
That is, the \textsl{Gaia} Catalog parallax and parallax uncertainty for any star are parameters of a form for the probability of the Gaia data given that star's parallax.
This is clean!
It permits any user of the Catalog to update their beliefs about that star's distance, or their beliefs about any other astrophysical inferences that involve that star's distance.
If the \textsl{Gaia} Catalog contained mean or median-of-posterior estimates or any other kind of biased estimators, the data would have been far harder (or even impossible) to use.

One result that motivated this \documentname{} is a ``cosmology with one galaxy'' claim \cite{onegalaxy}.
This claim is that there is significant information about some of the cosmological parameters---and especially the mean matter density of the Universe---in the measurable properties of a single galaxy.
This claim might be correct, but the paper does not demonstrate it.
The paper is not consistent with the likelihood principle; it does not ground its claim in the likelihood or in any proxy for the likelihood.
Instead it uses a machine-learning regression to substantiate its results; the issues with this will be discussed in \secref{sec:ml}.

I opened by noting that data analyses involve subjective choices.
There is another kind of subjectivity in play:
What method you use depends on what \emph{you want to do} with your data analysis.
The primary use case I am going to focus on is the case in which you have a data set and you have a parameter of interest, and you want to know \emph{what that particular data set has to say} about the parameter of interest.
There are other use cases.
Sometimes you are making the measurement because you want to make a \emph{prediction} for a future experiment.
That is, you want to know what some future data set will say about your parameter of interest.
Sometimes you are trying to make a \emph{decision} and your decision will depend on the value of the parameter, plus other things about your utility function (your economic goals, say).
We will discuss these alternative objectives a tiny bit in \secref{sec:bayes}, but mainly we will concentrate on answering the question ``what does my data\footnote{%
One of the questions I get asked most about data is whether the word ``data'' is singular or plural. I don't have a strong position. However, in this document I will attempt to treat the word ``data'' as a mass noun, like ``grass'' or ``hair.''}
say about my parameter of interest?''

My field is astrophysics, and many of the examples I give will be astrophysical.
Furthermore, the astrophysics community (now) is very Bayesian.
For this reason, the document that follows will seem to many of my astrophysics colleagues as oddly frequentist.
It isn't!
The likelihood principle is shared between Bayesians and frequentists:
The likelihood function is sufficient to deliver all information about the parameters coming from the data.
This is the key idea of Bayes's rule:
The prior is updated to the posterior with---and only with---the likelihood function.
Thus the question \emph{Do these data $Y$ deliver a measurement of parameter $\theta$?} must be answerable with a likelihood function---something like $p(Y\given\theta,I)$---if that likelihood function can be written down.
When the likelihood function cannot be written down, life is harder; we will address that case in \secref{sec:lfi}.

\section{The likelihood function}\label{sec:lf}
The problem setup will be that there is a list or block of data $Y$, a parameter (or list of parameters) of interest $\theta$, a list of nuisance parameters $\alpha$, and a large set of choices, assumptions, and background information $I$.
In contexts like this, there are many meanings of the word ``model,'' but for me this will be something that can be used to compute probabilities in the space of the data.
That is... HOGG
HOGG: Importantly: Nuisance parameters also!
HOGG: Importantly: Specific assumptions.

You can often (but not always) think of the likelihood function as a prediction for the expectation of the data plus a noise model.
For example, if you believe that the expectation for the data depends linearly on the parameters $(\theta, \alpha)$, and then in addition there is additive, Gaussian noise, the likelihood function becomes... HOGG
The likelihood function both tells you how you expect the data to change as you change the parameters, \emph{and} how precisely the data can be predicted, given the parameters.

I alluded to the likelihood principle in \secref{sec:intro}.
The principle is that everything about the data $Y$ that can possibly matter to the parameters $(\theta,\alpha)$ is contained in the likelihood function.
That is, if you have beliefs represented (accurately) a prior pdf $p(\theta,\alpha\given I)$ for the parameters $(\theta,\alpha)$,
and then you get data $Y$,
and then you want to update your beliefs to a posterior pdf $p(\theta,\alpha\given Y,I)$, the only legitimate procedure for updating your beliefs is to multiply by the likelihood function $p(Y\given\theta,\alpha,I)$:
\begin{align}
    p(\theta,\alpha\given Y,I) &= \frac{1}{Z}\,p(\theta,\alpha\given I)\,p(Y\given\theta,\alpha,I) ~,\label{eq:bayes}
\end{align}
where $Z$ is a normalization that ensures that $\int p(\theta,\alpha\given Y,I)\,\dd\theta\,\dd\alpha=1$.
This procedure \eqref{eq:bayes} is called ``Bayes rule'' and is the basis of Bayesian reasoning.
This is the \emph{only way} that the data are permitted to be used to update your beliefs, in Bayesian reasoning.
That's the likelihood principle.

But the likelihood principle has a role in frequentist statistics too.
The principle in frequentism is that everything about the data $Y$ that is relevant to the parameters $(\theta,\alpha)$ is encoded in the likelihood function.
That is, the likelihood function is a sufficient statistic for the data, when your goals relate (only) to the parameters $(\theta,\alpha)$.
Your best estimate $\hat{\theta}$ for the parameter $\theta$, and your uncertainty $\sigma_\theta$ on that parameter can all be derived from the likelihood function alone, for particular definitions of the word ``best.''\footnote{%
The definition of ``best'' estimator is out of scope for this document, but it relates to having minimum variance while also being asymptotically unbiased.}
The most important of these estimates is the maximum-likelihood estimate, found by maximizing the likelihood with respect to the parameter $\theta$ (and all the nuisance parameters $\alpha$).
Whatever estimate of $\theta$ you want to make, and independently of whether you (or your audience) is frequentist or Bayesian, you haven't measured your parameter $\theta$ with your data $Y$ if there isn't a peak in your likelihood function at some well-defined value of $\theta$.

In many contexts, both conceptually and for reasons of numerical precision, it is better to work with the logarithm $\mathscr{L}$ of the likelihood function;
\begin{align}
    \mathscr{L}(\theta,\alpha) &= \ln p(Y\given\theta,\alpha,I) ~.
\end{align}
When we take the log---and we always use natural logarithms here---we usually explicitly write the log likelihood in terms only of the parameters only.
The data are fixed, after all.

There are two very important properties of the likelihood function, when it comes to measurement.
The first is that differences in the natural logarithm of the likelihood have an absolute meaning:
A difference of 0.5 in natural log (a likelihood ratio of $\sqrt{\e}$) between two models is something akin to ``one sigma'' and a difference of 12.5 is something akin to ``five sigma'' (HOGG CHECK).
This (it turns out) means that the second derivative of the log likelihood function with respect to parameters is related to measurement uncertainty.
This is formalized in the Cram\'er--Rao bound \cite{cramer, rao} on the variance (and bias) of estimators; more on this in \secref{sec:inference}
It also means that any claim of a measurement requires a large difference in log likelihood between a model with a best-fit value of some parameter $\theta$ and a null model in which that parameter vanishes.

That is---and critical to everything that follows in this \documentname---any claim of a measurement of any parameter $\theta$ in data $Y$ requires not just a peak in the likelihood at some value of the parameter $\theta$, but a tall peak.
The peak in the log likelihood must be taller than its surroundings by log-likelihood differences much larger than unity.\footnote{Here, and everywhere in this document, the words ``log'' or ``logarithm'' or ``logarithmic'' always refer to natural logarithms in base $\e$.}
In particular, the log likelihood difference between the measured value of $\theta$ and the values that the investigator or project claims to ``rule out'' with their data must be substantially larger than unity.

The second important property of the likelihood function is that it is parameterization-invariant.
If the problem is reparameterized from parameters $(\theta,\alpha)$ to new parameters $\beta$ such that the transformations between $(\theta,\alpha)$ and $\beta$ are invertible (and the inverse is also invertible), then the value of the likelihood at $(\theta,\alpha)$ will be precisely equal to the likelihood at the corresponding value of $\beta$.
This invariance is extremely important to frequentist statistics, and does not hold for Bayesian posteriors.
The fact that Bayesian priors and posteriors do not have reparameterization invariance will come back up in \secref{sec:bayes}.

\section{Nuisance parameters}

Much of what follows is (effectively) about the nuisance parameters $\alpha$.
Nuisance parameters substantially complicate questions of measurement.
Many of the mistakes in the literature are mistakes in the handling of nuisance parameters.
This is in part because it is hard to get rid of the nuisance parameters without accidentally introducing information about the parameter of interest $\theta$.
Any information brought in that doesn't come directly from the data $Y$ is information that can deceive an investigator into mistakenly claiming a measurement or over-estimating the information in the data about the parameter of interest $\theta$.

How do we account for the nuisances $\alpha$ when we are claiming a measurement of the parameter of interest $\theta$?
For the committed Bayesians, there is only one answer, and that is to marginalize.
That is, you put a prior $p(\alpha\given I)$ on the nuisance parameters and integrate
\begin{align}
    p(\theta\given Y,I) &= \int p(\theta,\alpha\given Y,I)\,p(\alpha\given I)\,\dd\alpha ~,\label{eq:marginallf}
\end{align}
where the integral over $\alpha$ is, implicitly the integral over all of the nuisance parameter space, or the sum over all possible settings of $\alpha$ if $\alpha$ is discrete.
You might be tempted to conclude that we have a measurement of $\theta$ if there is a peak in this marginal likelihood.
That will turn out to be wrong, as we will show in \secref{sec:flat}, and related to ideas that come up in \secref{sec:ml}:
If the data contain lots of information about the nuisances $\alpha$ and the highest-likelihood values of the nuisances depend on the parameter of interest $\theta$, then these kinds of marginalizations will in general produce peaks in the marginal likelihood even when there is no information about $\theta$ in the data $Y$.
That's true even when those priors on the nuisances are flat, as we mention in \secref{sec:bayes} (HOGG DO WE?) and explicitly show in \secref{sec:flat}.

On the subject of nuisance parameters,
there is an extremely important idea in frequentist statistics, which is that of the \emph{profile likelihood}.
\begin{align}
    \mathscr{L}_\alpha(\theta) &= \max_\alpha \mathscr{L}(\theta,\alpha) ~,
\end{align}
where the subscript reminds us what we've profiled over, and the max operation means the following:
For every value of $\theta$, deliver the log likelihood for the maximum-likelihood value of $\alpha$ at that value of $\theta$.
This answers the questions of the form:
Is there \emph{any} setting of $\alpha$ that makes the data probable for this value of $\theta$?
That is, is it likely to make the data at this value of $\theta$ (when I have no opinions about $\alpha$)?
Bayesians can make a marginalized likelihood if they want to get rid of $\alpha$ (provided that they have a prior pdf on $\alpha$).
Frequentists don't usually\footnote{%
HOGG exception!}
have a pdf to apply on any parameters, so they can't marginalize; they have to optimize rather than marginalize over alpha.
Frequentists made a profile likelihood if they want to get rid of $\alpha$.\footnote{%
I like to joke that Bayesians integrate, and frequentists take derivatives. It isn't entirely a joke.}

Sometimes Bayesians do a really big integral, which is the likelihood marginalized over \emph{all} parameters,
and call it the \emph{evidence} or the \emph{fully marginalized likelihood}.
This is not relevant to our goals here, so I will leave my criticisms for a different rant.

HOGG: Something about true subjective Bayes if the community really does have shared beliefs about the nuisances. Then does the marginalized likelihood deliver a measurement? I think maybe it does?

\section{Statistical inference}\label{sec:inference}
Fundamentally, \emph{inference} is the estimation or measurement or prediction of $\theta$ given data $Y$.
For a frequentist, this involves an \emph{estimator}, which could be maximum-likelihood or otherwise.
For a Bayesian, this means making a likelihood function, and (perhaps) multiplying it by a prior and renormalizing to a posterior pdf (or sampling with MCMC \cite{mcmc}).
In what follows, for specificity, we will assume that the frequentists use maximum-likelihood estimators.
Bayesians don't necessarily have estimators, but sometimes a Bayesian will report a posterior mean or median or (wrongly\footnote{%
Recall my comment in HOGG FOOTNOTE REFERENCE that Bayesians integrate and frequentists differentiate. Bayesian beliefs are much better summarized with means or medians of posteriors than optima of posteriors: The optimum can be far from the bulk, oddly.})
a maximum of posterior.

Sometimes Bayesians like to think that, since they have a framework of prior pdf and posterior pdf, they don't need to ever choose an estimator.
There is an element of truth to this!
However even the Bayesians have to decide what number (or numbers or intervals) they \emph{report in the abstract of their paper} or in their summary slides about their parameter of interest $\theta$.
Thus even Bayesians have estimators at some level.
The qualities of estimators---in terms of bias and variance---are limited by a beautiful set of mathematics in the area of \emph{information theory}.
The most important, relevant result is the Cram\'er--Rao bound \cite{cramer, rao}, which limits the variance estimators.
This bound says that, given data $Y$, the best an unbiased estimator can do is set by the Fisher Information \cite{fisher},
One consequence of all this is that any claim about the ability of some data set $Y$ to measure or constrain some parameter or parameters of interest $\theta$ in the presence of nuisances $\alpha$ can all be boiled down to second derivatives of the log-likelihood $\mathscr{L}(\theta,\alpha)$.

Getting more specific:
It is possible to think about whether data $Y$ have provided a measurement of parameter $\theta$ by thinking about the \emph{uncertainty}\footnote{%
It's not an error bar or an error, it's an uncertainty. If it were an error, we would correct it.}
in the measurement of $\theta$.
Cram\'er--Rao guides us here:
This bound says that the best you can possibly do, in an unbiased-estimator sense, on your parameter of interest, given your data $Y$, is the square root of the inverse of the Fisher information.
That is, you start by computing the information tensor $C^{-1}_{(\theta,\alpha)}$
\begin{align}
    C^{-1}_{(\theta,\alpha)} &= - \frac{\dd^2\mathscr{L}}{\dd(\theta,\alpha)^2} ~.\label{eq:fisher}
\end{align}
This second-derivative tensor is $d\times d$, where $d$ is the total number of elements in the full set of parameters $(\theta,\alpha)$.
HOGG DO WE HAVE DEFINITIONS for the dimensionality of $\theta$ and $\alpha$?
Then you invert this matrix to get $C_{(\theta,\alpha)}$.
Then you take the (square) submatrix of $C_{(\theta,\alpha)}$ corresponding to $\theta$ only.\footnote{It is critical here that you invert before you take the submatrix. If you submatrix before you invert, you will get disastrously wrong uncertainty estimates. In that (wrong) case you have computed the best possible uncertainty under the (wrong) assumption that God told you the correct values of the nuisance parameters.}
If there is only one, scalar parameter of interest $\theta$, then this gives the uncertainty variance (the square of the uncertainty) on $\theta$.
If $\theta$ has more than one component, this submatrix is the variance tensor of the uncertainty ellipse in the $\theta$ space.
HOGG DO WE NEED SOME PICTURES OR DIAGRAMS?
Importantly, this bound sets the properties of any possible estimator.
If this Cram\'er--Rao (or Fisher-matrix) uncertainty is larger than what you need to claim a (say 5-sigma) measurement, then \emph{there is simply no sense in which data $Y$ have measured parameter $\theta$}.

Second derivatives like in \eqref{eq:fisher} sound hard!
But they aren't, for two reasons.
The first is that, when your model is linear, and your noise is Gaussian, this second derivative becomes an outer product of first derivatives (see, for example, \cite{fittingaline}).
The second is that we live in the future, in which programming languages like jax \cite{jax} give you analytic derivatives to second order for free.

Frequentists and Bayesians are all bound by Cram\'er--Rao.
However, Bayesians can do more than frequentists.
Bayesians can update beliefs, compute optimal betting odds on future outcomes, and optimize utility over decisions, integrating over all possible states of the world.
All these things are important, but they don't change the fundamental point that the likelihood function is sufficient for conveying the information in data $Y$ about parameter $\theta$ and thus the likelihood function is the only thing in play when a claim of measurement is being made.

\section{Isn't Bayes The Best Thing To Do (tm)?}\label{sec:bayes}
The most beautiful thing about Bayesian inference is that it is the provably correct way for an individual to reason about uncertain things.\footnote{%
The best explanation of this I know is the first Chapter of Jaynes's book \cite{jaynes}.}
The rules of probability theory are also the rules about reasoning about the plausibility of the things you know, or want to know.
For this reason, it makes sense for a scientist---or a human being---to reason in a Bayesian manner.
I would even say (and I have said elsewhere \cite{plausibility})
that, for the reason of this provable correctness, the whole scientific community ought to act, collectively, in a kind of Bayesian way.
It should have a collective set of beliefs and it should update those collective beliefs by (at least approximately) multiplying by a likelihood function for the new data it obtains each year.

Does this mean that \emph{every scientific paper} should describe a prior pdf, a likelihood, and a posterior obtained by Bayes's rule?
No, it does not.
The point of each scientific paper is \emph{not}---or not mainly---to describe to the community \emph{the authors' beliefs}.
The point of each scientific paper is to move forward the beliefs of the community of scientists.
That moving forward could be contributions to, or arguments about, the community's prior beliefs, the community's many likelihood functions, and the community's posterior beliefs.
Each paper has a small role in this program; it is not the case that each paper executes the whole program for the whole community.\footnote{%
There are many treatises on the philosophy of science.
I feel like we need a philosophical theory of the \emph{scientific paper} or of the \emph{individual scientific contribution}.}

When scientist A reads a paper written by scientist B, what does scientist A hope to get from that paper, and what did scientist B hope to provide in that paper?
I believe that scientist A wants to know if scientist A's beliefs need to be updated.
I believe that scientist B wants to influence the beliefs of scientist A.
In either case, what scientist A needs to get from the paper is not scientist B's posterior pdf.
What scientist A needs to get from the paper is scientist B's likelihood function (or, alternatively, scientist B's data; we will return to that idea below in secref... HOGG DO WE?).

If scientist A can understand clearly the assumptions going into scientist B's likelihood function,
and if scientist A broadly agrees with them,
then scientist A can simply update their beliefs by multiplying (approximately or as exactly as possible, depending on the nature of the work)
scientist A's prior pdf with scientist B's likelihood function.
And it is worthy of note that scientist A is much more prone to agree with the assumptions going into scientist B's likelihood function if scientist B's assumptions are aligned with community norms in the field.

When I say that scientist A will multiply their prior pdf by scientist B's likelihood function,
I don't necessarily mean that they will do so by writing a piece of code that imports or reimplements prior pdfs, imports the likelihood function, and literally does the multiply and representation of the output.
I mean something more approximate and vague.
But sometimes it \emph{literally is the case} that this code and import and multiply \emph{is} what scientist A does.
In the case from \secref{sec:intro} with the ESA \textsl{Gaia} Catalog, that is exactly what we have been doing.
And NASA created the \textsl{LAMBDA} archive in part to preserve executable likelihood functions in cosmology for exactly this purpose \cite{lambda}.

Of course this is a dream.
In many real cases, scientist A will not agree with the assumptions going into scientist B's likelihood function.
But usually scientist A will agree with a lot of those assumptions.
Then scientist B's paper is a template for making a new likelihood function with modified assumptions, which scientist A can then use to modify scientist A's beliefs.
This is a less direct use of scientist B's likelihood function, but still very useful.

The point of scientist B's paper is not to represent scientist B's beliefs, but instead to aid scientist A in \emph{updating} scientist A's beliefs, or maybe the beliefs of some community of which scientist A is a part.
Beliefs of individual scientists (or even whole scientific communities) are idiosyncratic and subjective.
If we want to update someone \emph{else's} beliefs, we must provide them with a likelihood function.
The point of (most of) the scientific literature is to communicate about (aspects of) likelihood functions.
When a paper multiplies a prior by a likelihood to produce a posterior it is---at best---demonstrating the effect of the data on typical beliefs.
At worst it is irrelevant to every individual reader.\footnote{%
HOGG WRITE about the DFM pov.}

HOGG: In what very specific sense is Bayes best?
It is best for bettors.
Thus Bayes is the right thing to do if you want to make a prediction.
HOGG Refer back here to \secref{sec:intro}.

HOGGG: There is another use case, which is decision-making.
Here again you might want to be Bayesian but then do expectations of utility....

The Cox proof about Bayesian reasoning is correct of course (it's a proof!).
You, personally ought to reason according to Bayes.
The scientific community ought to reason according to Bayes.
The important thing is that \emph{a measurement is not the same as a belief}.
Measurements are used to \emph{update} your beliefs.
Hence measurements are properties of your likelihood function, not properties of your posterior pdf.

\section{Machine learning or regression}\label{sec:ml}
There are many ways in which machine learning might enter into a project to measure a parameter $\theta$ given data $Y$.
Here we will only consider one, which is \emph{supervised regression}:
The setup for supervised regression is that there is a \emph{training set} $\setof{Y_i, \theta_i}_{i=1}^n$ of $n$ data instances $Y_i$ and corresponding labels $\theta_i$.
We are going to \emph{learn} a flexible function $f(Y;W)$, parameterized by a large block of weights $W$, that does a good job of predicting labels $\theta_i$ for training-set examples $Y_i$.
We will estimate our parameter of interest $\theta$ by executing the learned function on our data $Y$.

Ideally the labels $\theta_i$ in your training set are very reliable and accurate estimates for each training data object $Y_i$.
If they are noisy, pure regressions aren't a good idea; it makes more sense to go to models with a generative aspect (for example, CITE).
Ideally the data $Y_i$ in your training set are somehow ``drawn'' from the same distribution as the data $Y$ you have.
If they are not drawn from the same distribution, strong biases will occur.
Often it is believed that so long as the training data ``cover'' the test data (for example the test data live inside the convex hull of the training data), then things are fine.
This isn't true, and besides, once the data get large enough, coverage of that kind is impossible to satisfy.

It turns out that, even when the situation is ideal---%
even when the training labels are perfect, the data are drawn from the same distribution as the training data, and the function $f()$ has been given sufficient flexibility---the results of machine learning regressions are typically biased.
Full analysis of this is out of scope; it is discussed extensively elsewhere (for example, CITE).
However, it is relevant to the story being told here.
Machine learning regressions produce results with a lot of proerties similar to maximum a posteriori (maximum-of-posterior) parameter estimates, with the training data serving as an implicit prior.
\textbf{HOGG: Did we discuss this point in the previous section? If not, add it.}

Is a machine learning regression output a measurement?
No, it is not, or at least not in itself:
When a machine learning method returns a result, it is using information from your data $Y$
but also from all of the training data $\setof{Y_i, \theta_i}_{i=1}^n$.
\textbf{HOGG: What about looking at the distribution of $\theta$ in the training set and at the end? Or if you change the data slightly? Can't you answer the information theoretic questions that way? If not, why not?}

HOGG: MAKE SURE this section talks about a training set where a is strongly correlated with b, and the data have information about b. Then you will conclude that you can see a in the data!

\section{Can't I just use flat priors?}\label{sec:flat}
We need a peak in the likelihood function to claim a measurement.
But if we want to be Bayesian, and we want to use \emph{flat priors},\footnote{%
If you want to be Bayesian, there is almost no context in which you would want to use flat priors on any parameter you care about.
The Cox theorems that deliver Bayesian reasoning only apply when the Bayesian procedure is being used to update \emph{your beliefs}.
It is essentially impossible that your belief about anything you care about is well represented by a flat pdf.}
isn't it the case that any peak in the likelihood will lead to an identically shaped peak in the posterior pdf?

This sounds so naively and obviously true, what could possibly go wrong?
There are two things that are very wrong about this idea---the idea that if you use flat priors its the same as having just a likelihood function.
The first thing that is wrong is that (as we emphasized in \secref{sec:lf}) the likelihood function in reparameterization-invariant.
The posterior pdf is absolutely not reparameterization-invariant.
So if the prior is flat in some parameter, it is absolutely not flat in any non-trivial function of that parameter.
Even worse, if it is flat in some parameter, in any square-root or square or logarithm of that parameter, the prior is strongly peaked \emph{at one edge of the allowed range!}
That's demonstrated in FIGURE HOGG and definitely not what people usually have in mind when they think about using ``uninformative priors.''

The second thing that is wrong about this idea is that when the number of parameters gets large---larger than 2 say---it gets very very easy to believe that there is a peak in the likelihood function when there most definitely isn't.
To demonstrate this, we generate a very simple toy-likelihood and toy-posterior situation, in which all priors are flat, and in which it very very much looks like there is a peak in the likelihood function when there absolutely is not.
HOGG DETAILS AND FIGURES.

\section{Combining data samples}\label{sec:combining}
There are (at least) two cases for combining measurements.
In the first, there are two data sets, $Y_1$ and $Y_2$, and they both provide measurements of the parameters $\theta$ of interest, plus maybe nuisance parameters (which might differ between $Y_1$ and $Y_2$).
In this case, the goal might be to get the best possible measurement of the parameters $\theta$ given both data sets.
In the second, data set $Y_1$ measured parameter of interest $\theta_1$, and data set $Y_2$ measured parameter of interest $\theta_2$.
In this case, the goal might be to measure some new parameter $\beta$ that depends on the values $\theta_1$ and $\theta_2$.
In both of these cases, the most straightforward way to make the new measurement is by combining the two likelihood functions.

HOGG: Get explicit, with a notation and some equations.

HOGG: Assumption of independence of the data. NOTE that the sharing of nuisance parameters does not necessarily make the two data sets non-independent.

HOGG: The triviality of combining likelihood functions.

HOGG: The impossibility of combining posteriors unless they (and their associated priors) are continuous functions.

HOGG: The impossibility of reweighting posterior samples, despite our own work. CITE LIGO RESULTS TOO.

\section{Delivering useful catalogs of measurements}\label{sec:catalogs}
Imagine that you are making a \emph{lot} of measurements; so many that you are going to do some kind of ``data release'' of your measurements, so that other people in your scientific domain can use those measurements in their own scientific projects.
What kind of measurements or estimators do you want to use to build that set or list or catalog of measurements?

HOGG: FOR EXAMPLE..

For all the reasons given in \secref{sec:combining}, it is critical that the quantities given in your catalog can be used to reconstruct the likelihood functions, or approximations thereto.
HOGG BLAH BLAH. Also refer back to \secref{sec:intro} where the Gaia Catalog was discussed.

\section{What if I don't have a likelihood?}\label{sec:lfi}
Does the requirement that there be a peak in a sensible likelihood function imply that we can only determine whether or not we have a measurement by directly looking at the likelihood function?
Obviously, looking at the likelihood function is the best way to determine whether we have a peak in the likelihood function.
However, there are many circumstances in the contemporary sciences in which we can get some kind of estimate of our posterior pdf, but we cannot compute the likelihood function.

One example is in present-day cosmology, where one of the goals is to measure cosmological parameters using data on the large-scale structure (as traced by the positions of many millions of galaxies and quasars).
There are only a few important physical cosmological parameters, but there are (literally) billions of nuisance parameters (often wrongly called ``phases'') that must be chosen before the model makes a specific prediction for the positions of the galaxies.
Thus it is close to impossible, computationally, to compute the probability of the data given the parameters.
There are just far too many parameters.
At the same time, there are methods (known as likelihood-free inference, or simulation-based inference; \cite{abc, sbi}) for obtaining a posterior pdf for the important few parameters, without ever explicitly instantiating a likelihood function.
Is it impossible to securely claim a measurement in this case?
It is not.

HOGG: HOW TO SAFELY ratio the posterior and the prior. MODI POINT about approximating both.
This discussion should refer back to things in \secref{sec:flat}, where HOGG HOPES that we said things about how to safely look at the ratio of the prior to the posterior in high dimensions?

\section{Writing up the measurement in a paper}\label{sec:claim}
What does it take to write a paper that claims that a set of data $Y$ constrains or measures a parameter (or set of parameters) $\theta$?
It should be obvious at this point that it is a likelihood function, with a peak, and with a narrow enough uncertainty, or large enough Fisher Information, that the parameter is usefully measured
(if it is not possible to access any kind of likelihood function, see \secref{sec:lfi}).
But even more important than the requirement of a peak in a likelihood function is the requirement that the paper be absolutely clear and unambiguous about everything that was assumed and done.

The art of writing a good paper about a measurement is, as always, the art of writing clearly and with the audience in mind.
This means explaining every non-trivial decision and assumption that you have made---where ``non-trivial'' depends on the context, which in turn is set by the expectations of the audience---and showing that the likelihood function (or your method of measurement) flows directly from those decisions and assumptions.
I have taken the view recently that the decisions and assumptions should make up a stand-alone section in the papers I write (see, for example, \cite{frizzle}).
In addition, it is good practice to release all the code and data, such that the measurement can be reproduced, the assumptions can be explored, and the likelihood can be combined with others that follow (see \secref{sec:combining}).

A measurement will be more acceptable to the community the more consistent with community norms are the decisions and assumptions.
For example, imagine that in the data $Y$, the parameter of interest $\theta$ is strongly covariant with some nuisance parameter $\alpha_k$,
such that one cannot measure $\theta$ without having strong constraints on $\alpha_k$.
In this case it is possible for an investigator to deliver a measurement of $\theta$ if the investigator is willing to postulate a particular value or narrow range for $\alpha_k$.
That measurement of $\theta$ will only be accepted by the investigator's community if the community agrees that $\alpha_k$ does indeed live in or near that postulated range.
If that's controversial, then the investigator has not delivered what anyone would accept as a measurement.
For another example, if there is obviously non-Gaussian noise, but the measurement is only successful under an assumption of Gaussian noise, then there perhaps isn't a measurement in any real sense, even if there is a good peak in the assumed-Gaussian likelihood function.

HOGG: What do I write in the abstract of my paper? What uncertainty do I report? Answer this for all statistical philosophies.

HOGG: How many ``sigma'' is my measurement? This is a requirement in many fields of physics.

HOGG: Note that if you have a posterior peak but you can't see a peak in the likelihood function, it doesn't mean that you don't have a measurement! It just means that the scope of your likelihood function is too narrow. Maybe more data were involved. Be explicit about that and you will be okay.

\section{Discussion}\label{sec:discussion}
This might have sounded frequentist, but it is not!
This is a fully Bayesian position.
Actually, it is a position that doesn't make a commitment between frequentism and Bayesianism.

Many Bayesians feel that the only point of science is to update a posterior. Maybe?! But that would be the point of the literature taken as a whole. Not the point of every single paper you write.
If you want to support your Bayesian readers, give them a likelihood function!

If you put tons of work into making a likelihood function, why obscure it by multiplying it by a pointless and obscuring prior?

\paragraph{Acknowledgments}
It is a pleasure to thank
  Dan Foreman-Mackey (DeepMind) and
  Hans-Walter Rix (MPIA)
for existential discussions about these matters over the years, and
  Chirag Modi (NYU) and
  Hans-Walter Rix (MPIA)
for valuable discussions of the content of this particular \documentname.

\raggedright
\bibliographystyle{plain}
\bibliography{measurement}

\end{document}
